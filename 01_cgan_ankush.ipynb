{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-8A_FyqqlTW"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z_cqYIrb16lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65df2c8-39a6-4199-8271-53befad2bea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6zdRk3XlnI8d"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/x.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx-NUNzpnS6Z"
      },
      "outputs": [],
      "source": [
        "!mv /content/x/* /content/drive/MyDrive/prototype/Train/x/\n",
        "!mv /content/y/* /content/drive/MyDrive/prototype/Train/y/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NRJ_rGIXnidZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b938d3cb-b7af-4c20-b557-b190bf3ba592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(len(os.listdir('/content/drive/MyDrive/prototype/Train/x')))\n",
        "print(len(os.listdir('/content/drive/MyDrive/prototype/Train/y')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZRRuC1Dcqkll"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLt5R5-9lGOP"
      },
      "source": [
        "# Building Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mQ7wbn3lM1K"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class DownSample(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, apply_batchnorm=True):\n",
        "#         super(DownSample, self).__init__()\n",
        "#         layers = [\n",
        "#             nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "#         ]\n",
        "#         if apply_batchnorm:\n",
        "#             layers.append(nn.BatchNorm2d(out_channels))\n",
        "#         layers.append(nn.LeakyReLU(0.2))\n",
        "#         self.block = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.block(x)\n",
        "\n",
        "# # Define the upsampling block\n",
        "# class Upsample(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, apply_dropout=False):\n",
        "#         super(Upsample, self).__init__()\n",
        "#         layers = [\n",
        "#             nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU()\n",
        "#         ]\n",
        "#         if apply_dropout:\n",
        "#             layers.append(nn.Dropout(0.5))\n",
        "#         self.block = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x, skip_input):\n",
        "#         x = self.block(x)\n",
        "#         x = torch.cat((x, skip_input), 1)\n",
        "#         return x\n",
        "\n",
        "# # Generator adapted for SAR images\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, in_channels=1, out_channels=3):  # 1 input channel (SAR), 3 output channels (RGB)\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.down1 = DownSample(in_channels, 64)\n",
        "#         self.down2 = DownSample(64, 128)\n",
        "#         self.down3 = DownSample(128, 256)\n",
        "#         self.down4 = DownSample(256, 512)\n",
        "#         self.down5 = DownSample(512, 512)\n",
        "#         self.down6 = DownSample(512, 512)\n",
        "#         self.down7 = DownSample(512, 512)\n",
        "#         self.down8 = DownSample(512, 512)\n",
        "\n",
        "#         self.up1 = Upsample(512, 512)\n",
        "#         self.up2 = Upsample(1024, 512)\n",
        "#         self.up3 = Upsample(1024, 512)\n",
        "#         self.up4 = Upsample(1024, 512)\n",
        "#         self.up5 = Upsample(1024, 256)\n",
        "#         self.up6 = Upsample(512, 128)\n",
        "#         self.up7 = Upsample(256, 64)\n",
        "\n",
        "#         self.final = nn.Sequential(\n",
        "#             nn.Upsample(scale_factor=2),\n",
        "#             nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "#             nn.Conv2d(128, out_channels, kernel_size=4, padding=1),  # 3 output channels (RGB)\n",
        "#             nn.Tanh(),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # U-NET generator with skip connections from encoder to decoder\n",
        "#         d1 = self.down1(x)\n",
        "#         d2 = self.down2(d1)\n",
        "#         d3 = self.down3(d2)\n",
        "#         d4 = self.down4(d3)\n",
        "#         d5 = self.down5(d4)\n",
        "#         d6 = self.down6(d5)\n",
        "#         d7 = self.down7(d6)\n",
        "#         d8 = self.down8(d7)\n",
        "\n",
        "#         u1 = self.up1(d8, d7)\n",
        "#         u2 = self.up2(u1, d6)\n",
        "#         u3 = self.up3(u2, d5)\n",
        "#         u4 = self.up4(u3, d4)\n",
        "#         u5 = self.up5(u4, d3)\n",
        "#         u6 = self.up6(u5, d2)\n",
        "#         u7 = self.up7(u6, d1)\n",
        "#         u8 = self.final(u7)\n",
        "\n",
        "#         return u8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xd5A9hZCtPof"
      },
      "outputs": [],
      "source": [
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, apply_batchnorm=True):\n",
        "        super(DownSample, self).__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1, groups=in_channels, bias=False),  # Depthwise\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)  # Pointwise\n",
        "        ]\n",
        "        if apply_batchnorm:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# Define an upsampling block using interpolation followed by depthwise separable convolution\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, apply_dropout=False):\n",
        "        super(Upsample, self).__init__()\n",
        "        layers = [\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=4, padding='same', groups=in_channels, bias=False),  # Depthwise\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),  # Pointwise\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        if apply_dropout:\n",
        "            layers.append(nn.Dropout(0.5))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.block(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "        return x\n",
        "\n",
        "# Generator adapted for SAR images using depthwise separable convolutions\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.down1 = DownSample(in_channels, 64)\n",
        "        self.down2 = DownSample(64, 128)\n",
        "        self.down3 = DownSample(128, 256)\n",
        "        self.down4 = DownSample(256, 512)\n",
        "        self.down5 = DownSample(512, 512)\n",
        "        self.down6 = DownSample(512, 512)\n",
        "        self.down7 = DownSample(512, 512)\n",
        "        # self.down8 = DownSample(512, 512)\n",
        "\n",
        "        # self.up1 = Upsample(512, 512)\n",
        "        self.up2 = Upsample(512, 512)\n",
        "        self.up3 = Upsample(1024, 512)\n",
        "        self.up4 = Upsample(1024, 512)\n",
        "        self.up5 = Upsample(1024, 256)\n",
        "        self.up6 = Upsample(512, 128)\n",
        "        self.up7 = Upsample(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=4, padding='same', groups=128, bias=False),  # Depthwise\n",
        "            nn.Conv2d(128, out_channels, kernel_size=1, stride=1),  # Pointwise\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)  #128, 64\n",
        "        d2 = self.down2(d1) #64, 128\n",
        "        d3 = self.down3(d2) #32, 256\n",
        "        d4 = self.down4(d3) #16, 512\n",
        "        d5 = self.down5(d4) #8, 512\n",
        "        d6 = self.down6(d5) #4, 512\n",
        "        d7 = self.down7(d6) #2, 512\n",
        "        # d8 = self.down8(d7) #1, 512\n",
        "\n",
        "        # u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(d7, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1) #128x128x128c\n",
        "        u8 = self.final(u7)\n",
        "\n",
        "        return u8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ex5wfsqw84"
      },
      "source": [
        "# **Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cLoZEY90rnxk"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=4):\n",
        "        super(Discriminator, self).__init__()\n",
        "        def discriminator_block(in_filters, out_filters, stride=2, normalize=True):\n",
        "            layers = [\n",
        "                nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=stride, padding=1)\n",
        "                ]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels, 64, normalize=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512, stride=1),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mx4f4Slmy4AG"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PyZhPRr9OoVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfc046a-81eb-4ab0-cfe1-1238293e3917"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qPASJvyYFdi2"
      },
      "outputs": [],
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KZlQSdGLSDzq"
      },
      "outputs": [],
      "source": [
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IzsHcsLxFfbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f2f1b0-518a-4946-e4b5-71376348eef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7oklWXDMFtZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d5b25d-8428-4d69-8e2b-874a03333580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (down1): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (down2): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (down3): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (down4): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (down5): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (down6): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (down7): DownSample(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (up2): Upsample(\n",
            "    (block): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=512, bias=False)\n",
            "      (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (up3): Upsample(\n",
            "    (block): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (1): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=1024, bias=False)\n",
            "      (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (up4): Upsample(\n",
            "    (block): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (1): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=1024, bias=False)\n",
            "      (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (up5): Upsample(\n",
            "    (block): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (1): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=1024, bias=False)\n",
            "      (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (up6): Upsample(\n",
            "    (block): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=512, bias=False)\n",
            "      (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (up7): Upsample(\n",
            "    (block): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=256, bias=False)\n",
            "      (2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (final): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=128, bias=False)\n",
            "    (2): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pep47u9XIEa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d86d65e-fd5a-45ea-8797-9fae2d81c97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
            "  return F.conv2d(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 1, 128, 128]              16\n",
            "            Conv2d-2         [-1, 64, 128, 128]              64\n",
            "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
            "         LeakyReLU-4         [-1, 64, 128, 128]               0\n",
            "        DownSample-5         [-1, 64, 128, 128]               0\n",
            "            Conv2d-6           [-1, 64, 64, 64]           1,024\n",
            "            Conv2d-7          [-1, 128, 64, 64]           8,192\n",
            "       BatchNorm2d-8          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-9          [-1, 128, 64, 64]               0\n",
            "       DownSample-10          [-1, 128, 64, 64]               0\n",
            "           Conv2d-11          [-1, 128, 32, 32]           2,048\n",
            "           Conv2d-12          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-13          [-1, 256, 32, 32]             512\n",
            "        LeakyReLU-14          [-1, 256, 32, 32]               0\n",
            "       DownSample-15          [-1, 256, 32, 32]               0\n",
            "           Conv2d-16          [-1, 256, 16, 16]           4,096\n",
            "           Conv2d-17          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-18          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-19          [-1, 512, 16, 16]               0\n",
            "       DownSample-20          [-1, 512, 16, 16]               0\n",
            "           Conv2d-21            [-1, 512, 8, 8]           8,192\n",
            "           Conv2d-22            [-1, 512, 8, 8]         262,144\n",
            "      BatchNorm2d-23            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-24            [-1, 512, 8, 8]               0\n",
            "       DownSample-25            [-1, 512, 8, 8]               0\n",
            "           Conv2d-26            [-1, 512, 4, 4]           8,192\n",
            "           Conv2d-27            [-1, 512, 4, 4]         262,144\n",
            "      BatchNorm2d-28            [-1, 512, 4, 4]           1,024\n",
            "        LeakyReLU-29            [-1, 512, 4, 4]               0\n",
            "       DownSample-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 2, 2]           8,192\n",
            "           Conv2d-32            [-1, 512, 2, 2]         262,144\n",
            "      BatchNorm2d-33            [-1, 512, 2, 2]           1,024\n",
            "        LeakyReLU-34            [-1, 512, 2, 2]               0\n",
            "       DownSample-35            [-1, 512, 2, 2]               0\n",
            "         Upsample-36            [-1, 512, 4, 4]               0\n",
            "           Conv2d-37            [-1, 512, 4, 4]           8,192\n",
            "           Conv2d-38            [-1, 512, 4, 4]         262,144\n",
            "      BatchNorm2d-39            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-40            [-1, 512, 4, 4]               0\n",
            "         Upsample-41           [-1, 1024, 4, 4]               0\n",
            "         Upsample-42           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-43           [-1, 1024, 8, 8]          16,384\n",
            "           Conv2d-44            [-1, 512, 8, 8]         524,288\n",
            "      BatchNorm2d-45            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-46            [-1, 512, 8, 8]               0\n",
            "         Upsample-47           [-1, 1024, 8, 8]               0\n",
            "         Upsample-48         [-1, 1024, 16, 16]               0\n",
            "           Conv2d-49         [-1, 1024, 16, 16]          16,384\n",
            "           Conv2d-50          [-1, 512, 16, 16]         524,288\n",
            "      BatchNorm2d-51          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-52          [-1, 512, 16, 16]               0\n",
            "         Upsample-53         [-1, 1024, 16, 16]               0\n",
            "         Upsample-54         [-1, 1024, 32, 32]               0\n",
            "           Conv2d-55         [-1, 1024, 32, 32]          16,384\n",
            "           Conv2d-56          [-1, 256, 32, 32]         262,144\n",
            "      BatchNorm2d-57          [-1, 256, 32, 32]             512\n",
            "             ReLU-58          [-1, 256, 32, 32]               0\n",
            "         Upsample-59          [-1, 512, 32, 32]               0\n",
            "         Upsample-60          [-1, 512, 64, 64]               0\n",
            "           Conv2d-61          [-1, 512, 64, 64]           8,192\n",
            "           Conv2d-62          [-1, 128, 64, 64]          65,536\n",
            "      BatchNorm2d-63          [-1, 128, 64, 64]             256\n",
            "             ReLU-64          [-1, 128, 64, 64]               0\n",
            "         Upsample-65          [-1, 256, 64, 64]               0\n",
            "         Upsample-66        [-1, 256, 128, 128]               0\n",
            "           Conv2d-67        [-1, 256, 128, 128]           4,096\n",
            "           Conv2d-68         [-1, 64, 128, 128]          16,384\n",
            "      BatchNorm2d-69         [-1, 64, 128, 128]             128\n",
            "             ReLU-70         [-1, 64, 128, 128]               0\n",
            "         Upsample-71        [-1, 128, 128, 128]               0\n",
            "         Upsample-72        [-1, 128, 256, 256]               0\n",
            "           Conv2d-73        [-1, 128, 256, 256]           2,048\n",
            "           Conv2d-74          [-1, 3, 256, 256]             387\n",
            "             Tanh-75          [-1, 3, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 2,726,099\n",
            "Trainable params: 2,726,099\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 389.95\n",
            "Params size (MB): 10.40\n",
            "Estimated Total Size (MB): 400.60\n",
            "----------------------------------------------------------------\n",
            "torch.Size([1, 3, 256, 256])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           4,160\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
            "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,544\n",
            "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 31, 31]       2,097,664\n",
            "      BatchNorm2d-10          [-1, 512, 31, 31]           1,024\n",
            "        LeakyReLU-11          [-1, 512, 31, 31]               0\n",
            "           Conv2d-12            [-1, 1, 30, 30]           8,193\n",
            "================================================================\n",
            "Total params: 2,767,553\n",
            "Trainable params: 2,767,553\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 49152.00\n",
            "Forward/backward pass size (MB): 45.27\n",
            "Params size (MB): 10.56\n",
            "Estimated Total Size (MB): 49207.83\n",
            "----------------------------------------------------------------\n",
            "torch.Size([1, 1, 30, 30])\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(generator, input_size=(1, 256, 256))\n",
        "sample_input = torch.randn(1, 1, 256, 256).to(device)\n",
        "output = generator(sample_input)\n",
        "print(output.shape)\n",
        "\n",
        "\n",
        "summary(discriminator, input_size=[(1, 256, 256), (3, 256, 256)])\n",
        "img1 = torch.randn(1, 1, 256, 256).to(device)\n",
        "img2 = torch.randn(1, 3, 256, 256).to(device)\n",
        "output = discriminator(img1, img2)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QYS3Jg8rLKVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955fcb3d-6328-4673-fb87-ca45c6d196f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 10.46 MB\n",
            "Model size: 10.57 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "torch.save(generator.state_dict(), \"model.pth\")\n",
        "print(f\"Model size: {os.path.getsize('model.pth') / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "torch.save(discriminator.state_dict(), \"model.pth\")\n",
        "print(f\"Model size: {os.path.getsize('model.pth') / (1024 * 1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSjWDH1VuKsc"
      },
      "source": [
        "# **Defining Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cMytVBOjx5mL"
      },
      "outputs": [],
      "source": [
        "# # criterion_GAN = nn.MSELoss()\n",
        "# # criterion_pixelwise = nn.L1Loss()\n",
        "\n",
        "# \"\"\"# Chromatic Aberration Loss definition\n",
        "# class ChromaticAberrationLoss(nn.Module):\n",
        "#     def __init__(self, lambda_color=1.0, lambda_spatial=1.0, lambda_perceptual=1.0, lambda_edge=1.0):\n",
        "#         super(ChromaticAberrationLoss, self).__init__()\n",
        "#         self.lambda_color = lambda_color\n",
        "#         self.lambda_spatial = lambda_spatial\n",
        "#         self.lambda_perceptual = lambda_perceptual\n",
        "#         self.lambda_edge = lambda_edge\n",
        "\n",
        "#         # Pre-trained VGG19 model for perceptual loss\n",
        "#         vgg19 = models.vgg19(pretrained=True).features\n",
        "#         self.vgg19_block4_conv4 = nn.Sequential(*list(vgg19[:21])).eval()  # Block 4 conv 4\n",
        "#         for param in self.vgg19_block4_conv4.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#     # Convert from RGB to YUV (PyTorch version)\n",
        "#     def rgb_to_yuv(self, image):\n",
        "#         r, g, b = image[:, 0:1], image[:, 1:1], image[:, 2:1]\n",
        "#         y = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "#         u = -0.14713 * r - 0.28886 * g + 0.436 * b\n",
        "#         v = 0.615 * r - 0.51499 * g - 0.10001 * b\n",
        "#         return torch.cat([y, u, v], dim=1)\n",
        "\n",
        "#     # 1. Color Discrepancy Loss (L2 Norm in YUV space)\n",
        "#     def color_loss(self, y_true, y_pred):\n",
        "#         y_true_yuv = self.rgb_to_yuv(y_true)\n",
        "#         y_pred_yuv = self.rgb_to_yuv(y_pred)\n",
        "#         return torch.mean((y_true_yuv - y_pred_yuv) ** 2)\n",
        "\n",
        "#     # 2. Spatial Consistency Loss (L1 Norm between neighboring pixels)\n",
        "#     def spatial_loss(self, image):\n",
        "#         loss_vertical = torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
        "#         loss_horizontal = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]))\n",
        "#         return loss_vertical + loss_horizontal\n",
        "\n",
        "#     # 3. Perceptual Loss (using VGG19)\n",
        "#     def perceptual_loss(self, y_true, y_pred):\n",
        "#         y_true_vgg = self.vgg19_block4_conv4(y_true)\n",
        "#         y_pred_vgg = self.vgg19_block4_conv4(y_pred)\n",
        "#         return torch.mean((y_true_vgg - y_pred_vgg) ** 2)\n",
        "\n",
        "#     # 4. Edge-Aware Loss (gradient difference in edge areas)\n",
        "#     def edge_aware_loss(self, y_true, y_pred):\n",
        "#         grad_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
        "#         grad_true_y = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
        "#         grad_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
        "#         grad_pred_y = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
        "\n",
        "#         edge_loss = torch.mean(torch.abs(grad_true_x - grad_pred_x)) + \\\n",
        "#                     torch.mean(torch.abs(grad_true_y - grad_pred_y))\n",
        "#         return edge_loss\n",
        "\n",
        "#     # Total Chromatic Aberration Loss\n",
        "#     def forward(self, y_true, y_pred):\n",
        "#         color_loss_value = self.color_loss(y_true, y_pred)\n",
        "#         spatial_loss_value = self.spatial_loss(y_pred)\n",
        "#         perceptual_loss_value = self.perceptual_loss(y_true, y_pred)\n",
        "#         edge_loss_value = self.edge_aware_loss(y_true, y_pred)\n",
        "\n",
        "#         total_loss = (self.lambda_color * color_loss_value) + \\\n",
        "#                      (self.lambda_spatial * spatial_loss_value) + \\\n",
        "#                      (self.lambda_perceptual * perceptual_loss_value) + \\\n",
        "#                      (self.lambda_edge * edge_loss_value)\n",
        "\n",
        "#         return total_loss\"\"\"\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchvision import models\n",
        "\n",
        "# # Convert RGB to LAB using PyTorch\n",
        "# def rgb_to_lab(image):\n",
        "#     # The conversion logic from RGB to LAB will be approximated.\n",
        "#     # For simplicity, here we assume the image is normalized to [0, 1].\n",
        "#     image = (image + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
        "#     image = image.clamp(0, 1)  # Ensure pixel values are within [0, 1]\n",
        "\n",
        "#     # You can use a library like OpenCV to convert to LAB, but here's a placeholder\n",
        "#     # You would need to use `cv2.cvtColor(image, cv2.COLOR_RGB2LAB)` with actual data.\n",
        "#     # Placeholder conversion: Assume yuv is LAB (for simplicity)\n",
        "#     return image  # This would be replaced by actual LAB conversion logic\n",
        "\n",
        "# # # Chromatic Aberration Loss in PyTorch\n",
        "# # class ChromaticAberrationLoss(nn.Module):\n",
        "# #     def __init__(self, lambda_color=1.0, lambda_spatial=1.0, lambda_perceptual=1.0, lambda_edge=1.0):\n",
        "# #         super(ChromaticAberrationLoss, self).__init__()\n",
        "# #         self.lambda_color = lambda_color\n",
        "# #         self.lambda_spatial = lambda_spatial\n",
        "# #         self.lambda_perceptual = lambda_perceptual\n",
        "# #         self.lambda_edge = lambda_edge\n",
        "\n",
        "# #         # Load the pre-trained VGG19 model for perceptual loss\n",
        "# #         vgg = models.vgg19(pretrained=True).features\n",
        "# #         self.vgg = nn.Sequential(*list(vgg.children())[:22])  # Extract up to 'block4_conv4'\n",
        "# #         for param in self.vgg.parameters():\n",
        "# #             param.requires_grad = False  # Freeze VGG19 parameters\n",
        "\n",
        "# #     def forward(self, y_true, y_pred):\n",
        "# #         # 1. Color Discrepancy Loss (L2 Norm in LAB space)\n",
        "# #         y_true_lab = rgb_to_lab(y_true)\n",
        "# #         y_pred_lab = rgb_to_lab(y_pred)\n",
        "# #         color_loss = F.mse_loss(y_true_lab, y_pred_lab)\n",
        "\n",
        "# #         # 2. Spatial Consistency Loss (L1 Norm between neighboring pixels)\n",
        "# #         def spatial_loss(image):\n",
        "# #             loss_x = F.l1_loss(image[:, :, :-1, :], image[:, :, 1:, :])\n",
        "# #             loss_y = F.l1_loss(image[:, :, :, :-1], image[:, :, :, 1:])\n",
        "# #             return loss_x + loss_y\n",
        "\n",
        "# #         spatial_loss_value = spatial_loss(y_pred)\n",
        "\n",
        "# #         # 3. Perceptual Loss using VGG19 features\n",
        "# #         def perceptual_loss(y_true, y_pred):\n",
        "# #             y_true_vgg = self.vgg(y_true)\n",
        "# #             y_pred_vgg = self.vgg(y_pred)\n",
        "# #             return F.mse_loss(y_true_vgg, y_pred_vgg)\n",
        "\n",
        "# #         perceptual_loss_value = perceptual_loss(y_true, y_pred)\n",
        "\n",
        "# #         # 4. Edge-Aware Loss (gradient difference in edge areas)\n",
        "# #         def edge_aware_loss(y_true, y_pred):\n",
        "# #             grad_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
        "# #             grad_true_y = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
        "# #             grad_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
        "# #             grad_pred_y = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
        "# #             edge_loss = F.l1_loss(grad_true_x, grad_pred_x) + F.l1_loss(grad_true_y, grad_pred_y)\n",
        "# #             return edge_loss\n",
        "\n",
        "# #         edge_loss_value = edge_aware_loss(y_true, y_pred)\n",
        "\n",
        "# #         # Total Chromatic Aberration Loss\n",
        "# #         total_loss = (self.lambda_color * color_loss) + \\\n",
        "# #                      (self.lambda_spatial * spatial_loss_value) + \\\n",
        "# #                      (self.lambda_perceptual * perceptual_loss_value) + \\\n",
        "# #                      (self.lambda_edge * edge_loss_value)\n",
        "\n",
        "# #         return total_loss\n",
        "\n",
        "# class ChromaticAberrationLoss(nn.Module):\n",
        "#     def __init__(self, device, lambda_color=1.0, lambda_spatial=1.0, lambda_perceptual=1.0, lambda_edge=1.0):\n",
        "#         super(ChromaticAberrationLoss, self).__init__()\n",
        "#         self.lambda_color = lambda_color\n",
        "#         self.lambda_spatial = lambda_spatial\n",
        "#         self.lambda_perceptual = lambda_perceptual\n",
        "#         self.lambda_edge = lambda_edge\n",
        "\n",
        "#         # Load the pre-trained VGG19 model for perceptual loss and move it to the appropriate device\n",
        "#         vgg = models.vgg19(pretrained=True).features\n",
        "#         self.vgg = nn.Sequential(*list(vgg.children())[15:22])  # Extract up to 'block4_conv4'\n",
        "#         self.vgg.to(device)  # Move the VGG model to the appropriate device (GPU/CPU)\n",
        "\n",
        "#         for param in self.vgg.parameters():\n",
        "#             param.requires_grad = False  # Freeze VGG19 parameters\n",
        "\n",
        "#         self.device = device  # Store device for later use\n",
        "\n",
        "#     def forward(self, y_true, y_pred):\n",
        "#         # Move inputs to the same device as the model (VGG)\n",
        "#         y_true = y_true.to(self.device)\n",
        "#         y_pred = y_pred.to(self.device)\n",
        "\n",
        "#         # 1. Color Discrepancy Loss (L2 Norm in LAB space)\n",
        "#         y_true_lab = rgb_to_lab(y_true)\n",
        "#         y_pred_lab = rgb_to_lab(y_pred)\n",
        "#         color_loss = F.mse_loss(y_true_lab, y_pred_lab)\n",
        "\n",
        "#         # 2. Spatial Consistency Loss (L1 Norm between neighboring pixels)\n",
        "#         def spatial_loss(image):\n",
        "#             loss_x = F.l1_loss(image[:, :, :-1, :], image[:, :, 1:, :])\n",
        "#             loss_y = F.l1_loss(image[:, :, :, :-1], image[:, :, :, 1:])\n",
        "#             return loss_x + loss_y\n",
        "\n",
        "#         spatial_loss_value = spatial_loss(y_pred)\n",
        "\n",
        "#         # 3. Perceptual Loss using VGG19 features\n",
        "#         def perceptual_loss(y_true, y_pred):\n",
        "#             y_true_vgg = self.vgg(y_true)\n",
        "#             y_pred_vgg = self.vgg(y_pred)\n",
        "#             return F.mse_loss(y_true_vgg, y_pred_vgg)\n",
        "\n",
        "#         perceptual_loss_value = perceptual_loss(y_true, y_pred)\n",
        "\n",
        "#         # 4. Edge-Aware Loss (gradient difference in edge areas)\n",
        "#         def edge_aware_loss(y_true, y_pred):\n",
        "#             grad_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
        "#             grad_true_y = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
        "#             grad_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
        "#             grad_pred_y = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
        "#             edge_loss = F.l1_loss(grad_true_x, grad_pred_x) + F.l1_loss(grad_true_y, grad_pred_y)\n",
        "#             return edge_loss\n",
        "\n",
        "#         edge_loss_value = edge_aware_loss(y_true, y_pred)\n",
        "\n",
        "#         # Total Chromatic Aberration Loss\n",
        "#         total_loss = (self.lambda_color * color_loss) + \\\n",
        "#                      (self.lambda_spatial * spatial_loss_value) + \\\n",
        "#                      (self.lambda_perceptual * perceptual_loss_value) + \\\n",
        "#                      (self.lambda_edge * edge_loss_value)\n",
        "\n",
        "#         return total_loss\n",
        "\n",
        "# criterion_GAN = nn.MSELoss()\n",
        "# criterion_pixelwise = ChromaticAberrationLoss(device, lambda_color=1.0, lambda_spatial=1.0, lambda_perceptual=1.0, lambda_edge=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "\n",
        "class ChromaticAberrationLoss(nn.Module):\n",
        "    def __init__(self, device, lambda_L1=1.0, lambda_CA=1.0, lambda_perceptual=1.0, kernel_size=5, sigma=3.0):\n",
        "        super(ChromaticAberrationLoss, self).__init__()\n",
        "\n",
        "        # Initialize weights for the loss terms\n",
        "        self.lambda_L1 = lambda_L1\n",
        "        self.lambda_CA = lambda_CA\n",
        "        self.lambda_perceptual = lambda_perceptual\n",
        "\n",
        "        # L1 Loss\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "\n",
        "        # Gaussian Blur using torchvision.transforms\n",
        "        self.gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
        "\n",
        "        # VGG19 for perceptual loss\n",
        "        vgg = models.vgg19(pretrained=True).features\n",
        "        self.vgg19 = vgg[:21].eval().to(device)  # Use layers up to block4_conv4\n",
        "        for param in self.vgg19.parameters():\n",
        "            param.requires_grad = False  # Freeze VGG19\n",
        "\n",
        "    def forward(self, generated, target):\n",
        "        \"\"\"\n",
        "        Calculate the combined loss.\n",
        "        \"\"\"\n",
        "        # L1 Loss\n",
        "        loss_L1 = self.lambda_L1 * self.l1_loss(generated, target)\n",
        "\n",
        "        # Chromatic Aberration Loss\n",
        "        loss_CA = self.lambda_CA * self.chromatic_aberration_loss(generated, target)\n",
        "\n",
        "        # Perceptual Loss\n",
        "        loss_perceptual = self.lambda_perceptual * self.perceptual_loss(generated, target)\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss = loss_L1 + loss_CA + loss_perceptual\n",
        "        return total_loss\n",
        "\n",
        "    def chromatic_aberration_loss(self, generated, target):\n",
        "        \"\"\"\n",
        "        Calculate chromatic aberration loss using Gaussian blur.\n",
        "        \"\"\"\n",
        "        blurred_gen = self.gaussian_blur(generated)\n",
        "        blurred_target = self.gaussian_blur(target)\n",
        "        return F.mse_loss(blurred_gen, blurred_target)\n",
        "\n",
        "    def perceptual_loss(self, generated, target):\n",
        "        \"\"\"\n",
        "        Calculate perceptual loss using pre-trained VGG19.\n",
        "        \"\"\"\n",
        "        gen_features = self.vgg19(generated)\n",
        "        target_features = self.vgg19(target)\n",
        "        return F.mse_loss(gen_features, target_features)\n",
        "\n",
        "criterion_GAN = nn.BCEWithLogitsLoss()\n",
        "criterion_pixelwise = ChromaticAberrationLoss(device, lambda_L1=1.0, lambda_CA=1.0, lambda_perceptual=1.0, kernel_size=5, sigma=3.0)"
      ],
      "metadata": {
        "id": "BvETzbfB1AES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b327ff-e767-43e2-d96b-8454bf38a926"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|| 548M/548M [00:03<00:00, 176MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuNJRQdoBrOc"
      },
      "source": [
        "# Optimizers and Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0ZNTJ0ZiBwtu"
      },
      "outputs": [],
      "source": [
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxwNepZwucoW"
      },
      "source": [
        "# Dataset class for handling SAR input and DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sF9PP0KeCLi6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, SAR_root, color_root=None, transforms_=None):\n",
        "        self.SAR_root = SAR_root\n",
        "        self.color_root = color_root\n",
        "        self.transforms = transforms_\n",
        "\n",
        "        # Get sorted list of SAR images\n",
        "        self.SAR_images = sorted([f for f in os.listdir(SAR_root) if f.endswith('.png')])\n",
        "\n",
        "        # If color images are provided, get sorted list of color images\n",
        "        if color_root:\n",
        "            self.color_images = sorted([f for f in os.listdir(color_root) if f.endswith('.png')])\n",
        "            # Ensure the number of SAR and color images are the same\n",
        "            assert len(self.SAR_images) == len(self.color_images), \"Mismatch between SAR and color image count\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.SAR_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load SAR image\n",
        "        SAR_image_path = os.path.join(self.SAR_root, self.SAR_images[idx])\n",
        "        SAR_image = Image.open(SAR_image_path).convert('L')  # Grayscale (1 channel)\n",
        "\n",
        "        if self.color_root:\n",
        "            # Load corresponding color image\n",
        "            color_image_path = os.path.join(self.color_root, self.color_images[idx])\n",
        "            color_image = Image.open(color_image_path).convert('RGB')  # RGB (3 channels)\n",
        "\n",
        "            # Apply transformations if available\n",
        "            if self.transforms:\n",
        "                SAR_image = self.transforms(SAR_image)\n",
        "                color_image = self.transforms(color_image)\n",
        "\n",
        "            return SAR_image, color_image\n",
        "        else:\n",
        "            # Apply transformations to SAR image only\n",
        "            if self.transforms:\n",
        "                SAR_image = self.transforms(SAR_image)\n",
        "            return SAR_image\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize to match your model input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize (use 3 channels normalization if needed for color)\n",
        "])\n",
        "\n",
        "# DataLoader for training\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(\n",
        "        SAR_root=\"/content/drive/MyDrive/prototype/Train/x\",  # Path to SAR images\n",
        "        color_root=\"/content/drive/MyDrive/prototype/Train/y\",  # Path to color images\n",
        "        transforms_=transform\n",
        "    ),\n",
        "    batch_size=32,  # Adjust the batch size as needed\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6_e-A-eCrgt"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aq7fVM0AwLrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fb279d-bd3e-4591-9865-36921cdf57ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/200:  22%|       | 7/32 [03:49<12:30, 30.02s/it, D loss=0.9382, G loss=6.9745, ETA=820.31s]"
          ]
        }
      ],
      "source": [
        "# Paths to save the best models\n",
        "save_path_generator = '/content/drive/MyDrive/prototype2/generator2.pth'\n",
        "save_path_discriminator = '/content/drive/MyDrive/prototype2/discriminator2.pth'\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_loss_G = float('inf')\n",
        "best_loss_D = float('inf')\n",
        "no_improvement_G = 0\n",
        "no_improvement_D = 0\n",
        "no_of_epochs = 200\n",
        "\n",
        "# Get the total number of batches\n",
        "total_batches = len(dataloader)\n",
        "\n",
        "# Start the training loop\n",
        "for epoch in range(no_of_epochs):\n",
        "    start_time = time.time()  # Record start time for ETA calculation\n",
        "    epoch_loss_G = 0\n",
        "    epoch_loss_D = 0\n",
        "\n",
        "    dataloader_tqdm = tqdm(dataloader, desc=f'Epoch {epoch+1}/{no_of_epochs}', leave=False)\n",
        "\n",
        "    for i, (SAR_imgs, color_imgs) in enumerate(dataloader_tqdm):\n",
        "        SAR_imgs = SAR_imgs.to(device)\n",
        "        color_imgs = color_imgs.to(device)\n",
        "\n",
        "        valid = torch.ones((SAR_imgs.size(0), 1, 30, 30), requires_grad=False).to(device)  # Adjust to match image size\n",
        "        fake = torch.zeros((SAR_imgs.size(0), 1, 30, 30), requires_grad=False).to(device)  # Adjust to match image size\n",
        "\n",
        "        # ------------------\n",
        "        # Train Generator\n",
        "        # ------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        fake_imgs = generator(SAR_imgs)\n",
        "        # Resize fake images to match color images size if necessary\n",
        "        fake_imgs_resized = F.interpolate(fake_imgs, size=color_imgs.size()[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Ensure that fake_imgs_resized and color_imgs have the same size\n",
        "        assert fake_imgs_resized.size() == color_imgs.size(), f\"Size mismatch: {fake_imgs_resized.size()} vs {color_imgs.size()}\"\n",
        "\n",
        "        # GAN loss (Discriminator should classify fake images as valid)\n",
        "        loss_GAN = criterion_GAN(discriminator(fake_imgs_resized, SAR_imgs), valid)\n",
        "\n",
        "        # Pixel-wise loss\n",
        "        loss_pixelwise = criterion_pixelwise(fake_imgs_resized, color_imgs)\n",
        "\n",
        "        # Total generator loss\n",
        "        loss_G = loss_GAN + loss_pixelwise\n",
        "        loss_G.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ------------------\n",
        "        # Train Discriminator\n",
        "        # ------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Real images (Discriminator should classify real images as valid)\n",
        "        loss_real = criterion_GAN(discriminator(color_imgs, SAR_imgs), valid)\n",
        "\n",
        "        # Fake images (Discriminator should classify generated images as fake)\n",
        "        loss_fake = criterion_GAN(discriminator(fake_imgs_resized.detach(), SAR_imgs), fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        loss_D = 0.5 * (loss_real + loss_fake)\n",
        "        loss_D.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
        "        optimizer_D.step()\n",
        "\n",
        "        epoch_loss_G += loss_G.item()\n",
        "        epoch_loss_D += loss_D.item()\n",
        "\n",
        "        # Calculate elapsed time and ETA\n",
        "        elapsed_time = time.time() - start_time\n",
        "        avg_time_per_batch = elapsed_time / (i + 1)\n",
        "        remaining_batches = total_batches - (i + 1)\n",
        "        eta = avg_time_per_batch * remaining_batches\n",
        "\n",
        "        # Update the tqdm description with ETA\n",
        "        dataloader_tqdm.set_postfix({\n",
        "            'D loss': f'{loss_D.item():.4f}',\n",
        "            'G loss': f'{loss_G.item():.4f}',\n",
        "            'ETA': f'{eta:.2f}s'\n",
        "        })\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss_G = epoch_loss_G / len(dataloader)\n",
        "    avg_loss_D = epoch_loss_D / len(dataloader)\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch+1}/{no_of_epochs}] [Avg D loss: {avg_loss_D:.4f}] [Avg G loss: {avg_loss_G:.4f}]\")\n",
        "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds | ETA: {eta:.2f} seconds\")\n",
        "\n",
        "    # Early stopping and model checkpointing\n",
        "    if avg_loss_G < best_loss_G:\n",
        "        best_loss_G = avg_loss_G\n",
        "        no_improvement_G = 0\n",
        "        best_generator_state = generator.state_dict()\n",
        "    else:\n",
        "        no_improvement_G += 1\n",
        "\n",
        "    if avg_loss_D < best_loss_D:\n",
        "        best_loss_D = avg_loss_D\n",
        "        no_improvement_D = 0\n",
        "        best_discriminator_state = discriminator.state_dict()\n",
        "    else:\n",
        "        no_improvement_D += 1\n",
        "\n",
        "    # Check for early stopping\n",
        "    if no_improvement_G >= patience and no_improvement_D >= patience:\n",
        "        print(\"Early stopping triggered. Training stopped.\")\n",
        "        break\n",
        "\n",
        "# Save the final best models with architecture and optimizer state\n",
        "torch.save({\n",
        "    'model_state_dict': best_generator_state,\n",
        "    'optimizer_state_dict': optimizer_G.state_dict(),\n",
        "    'loss': best_loss_G,\n",
        "}, save_path_generator)\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': best_discriminator_state,\n",
        "    'optimizer_state_dict': optimizer_D.state_dict(),\n",
        "    'loss': best_loss_D,\n",
        "}, save_path_discriminator)\n",
        "\n",
        "print(\"Training complete. Best models saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ4QIo2WQYDr"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrfkM1OiQaYc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Load the generator model for testing\n",
        "checkpoint_G = torch.load(\"/content/drive/MyDrive/prototype/Models/generator/generator.pth\")\n",
        "generator.load_state_dict(checkpoint_G['model_state_dict'])\n",
        "optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
        "generator.eval()  # Set the generator to evaluation mode\n",
        "\n",
        "\n",
        "# Load test dataset for inference\n",
        "test_dataset = ImageDataset(\n",
        "    SAR_root=\"/content/drive/MyDrive/prototype/Test\",\n",
        "    color_root=None,  # Not needed for testing\n",
        "    transforms_=transform\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Function to test the generator and display images\n",
        "def test_generator(generator, test_loader, num_images=5):\n",
        "    generator.eval()  # Set the generator to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "        for i, SAR_imgs in enumerate(test_loader):\n",
        "            SAR_imgs = SAR_imgs.to(device)  # Move SAR images to the device\n",
        "            generated_imgs = generator(SAR_imgs)\n",
        "            generated_imgs = 0.5 * (generated_imgs + 1)  # Denormalize from [-1, 1] to [0, 1]\n",
        "            SAR_imgs = SAR_imgs.cpu()\n",
        "            generated_imgs = generated_imgs.cpu()\n",
        "\n",
        "            # Display the first few images in the batch\n",
        "            for j in range(min(num_images, SAR_imgs.size(0))):\n",
        "                fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "                # Input SAR image\n",
        "                axes[0].imshow(SAR_imgs[j].squeeze(0), cmap='gray')  # Display as grayscale\n",
        "                axes[0].set_title('Input SAR Image')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Output colorized image\n",
        "                axes[1].imshow(transforms.ToPILImage()(generated_imgs[j]))\n",
        "                axes[1].set_title('Generated Color Image')\n",
        "                axes[1].axis('off')\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "# Run testing and visualization\n",
        "test_generator(generator, test_dataloader, num_images=5)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z135Ab_lrCW2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Create Testpred directory if it doesn't exist\n",
        "save_dir = \"/content/drive/MyDrive/prototype2/Testpred\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load the generator model for testing\n",
        "checkpoint_G = torch.load(\"/content/drive/MyDrive/prototype2/generator2.pth\", map_location=torch.device('cpu'))\n",
        "generator.load_state_dict(checkpoint_G['model_state_dict'])\n",
        "optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
        "generator.eval()  # Set the generator to evaluation mode\n",
        "\n",
        "\n",
        "# Load test dataset for inference\n",
        "test_dataset = ImageDataset(\n",
        "    SAR_root=\"/content/drive/MyDrive/prototype/Test\",\n",
        "    color_root=None,  # Not needed for testing\n",
        "    transforms_=transform\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "# Function to test the generator, display images, and save them\n",
        "def test_generator(generator, test_loader, num_images=5, save_dir=save_dir):\n",
        "    generator.eval()  # Set the generator to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "        for i, SAR_imgs in enumerate(test_loader):\n",
        "            SAR_imgs = SAR_imgs.to(device)  # Move SAR images to the device\n",
        "            generated_imgs = generator(SAR_imgs)\n",
        "            generated_imgs = 0.5 * (generated_imgs + 1)  # Denormalize from [-1, 1] to [0, 1]\n",
        "            SAR_imgs = SAR_imgs.cpu()\n",
        "            generated_imgs = generated_imgs.cpu()\n",
        "\n",
        "            # Display the first few images in the batch\n",
        "            for j in range(min(num_images, SAR_imgs.size(0))):\n",
        "                fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "                # Input SAR image\n",
        "                axes[0].imshow(SAR_imgs[j].squeeze(0), cmap='gray')  # Display as grayscale\n",
        "                axes[0].set_title('Input SAR Image')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Output colorized image\n",
        "                axes[1].imshow(transforms.ToPILImage()(generated_imgs[j]))\n",
        "                axes[1].set_title('Generated Color Image')\n",
        "                axes[1].axis('off')\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                # Save the generated image\n",
        "                generated_img_pil = transforms.ToPILImage()(generated_imgs[j])\n",
        "                img_save_path = os.path.join(save_dir, f'generated_image_{i}_{j}.png')\n",
        "                generated_img_pil.save(img_save_path)\n",
        "                print(f\"Saved: {img_save_path}\")\n",
        "\n",
        "# Run testing, visualization, and saving\n",
        "test_generator(generator, test_dataloader, num_images=6)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}