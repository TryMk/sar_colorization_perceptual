{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4QqNgUVXYLXf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nu9LPGCVYhzf"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/sardata_small.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IvLUGASHYA5r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# import cv2\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "# import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import manual_seed\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision.io import decode_image\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JENxwafLYA50",
        "outputId": "03a0c71c-2f41-421d-8547-7b6172b0e9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "CSV_FILE_PATH = './sardata_small.csv'\n",
        "IMAGE_DIR_SAR = './sardata_small/s1'\n",
        "IMAGE_DIR_COL = './sardata_small/s2'\n",
        "IMAGE_SIZE = (256, 256)\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42\n",
        "manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "irVLpJpZYA53"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv(CSV_FILE_PATH)\n",
        "\n",
        "# train_df, test_df = train_test_split(data_df, test_size=0.15, random_state=SEED, shuffle=True, stratify=data_df['type'])\n",
        "# print(train_df.groupby('type').count())\n",
        "# print(test_df.groupby('type').count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ij3r2HK1YA54"
      },
      "outputs": [],
      "source": [
        "class SarColorDataset(Dataset):\n",
        "    def __init__(self, data_df, image_dir_sar, image_dir_col, transform_sar=None, transform_col=None):\n",
        "        self.data_df = data_df\n",
        "        self.image_dir_sar = image_dir_sar\n",
        "        self.image_dir_col = image_dir_col\n",
        "        self.transform_sar = transform_sar\n",
        "        self.transform_col = transform_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data_df.iloc[index]\n",
        "        label = row['type']\n",
        "\n",
        "        image_path_sar = os.path.join(self.image_dir_sar, row['s1_image'])\n",
        "        image_path_col = os.path.join(self.image_dir_col, row['s2_image'])\n",
        "\n",
        "        # image_sar = decode_image(image_path_sar, mode='GRAY')\n",
        "        # image_col = decode_image(image_path_col, mode='RGB')\n",
        "\n",
        "        # image_sar = cv2.imread(image_path_sar, cv2.IMREAD_GRAYSCALE)\n",
        "        # image_col = cv2.imread(image_path_col, cv2.IMREAD_COLOR_RGB)\n",
        "\n",
        "        image_sar = Image.open(image_path_sar).convert('L')\n",
        "        image_col = Image.open(image_path_col).convert('RGB')\n",
        "\n",
        "        if self.transform_sar:\n",
        "            image_sar = self.transform_sar(image_sar)\n",
        "\n",
        "        if self.transform_col:\n",
        "            image_col = self.transform_col(image_col)\n",
        "\n",
        "        return image_sar, image_col, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d74VXSB0YA56"
      },
      "outputs": [],
      "source": [
        "transform_sar = transforms.Compose([\n",
        "    transforms.Resize(size=IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,), inplace=False),\n",
        "])\n",
        "\n",
        "transform_col = transforms.Compose([\n",
        "    transforms.Resize(size=IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), inplace=False),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xVC3ctFqYA57"
      },
      "outputs": [],
      "source": [
        "dataset = SarColorDataset(\n",
        "    data_df=data_df,\n",
        "    image_dir_sar=IMAGE_DIR_SAR,\n",
        "    image_dir_col=IMAGE_DIR_COL,\n",
        "    transform_sar=transform_sar,\n",
        "    transform_col=transform_col,\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd2SuN6ykayq",
        "outputId": "0c51482c-5569-4556-ff79-4fce47035c68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ngXWj5H3YA59"
      },
      "outputs": [],
      "source": [
        "class DownSample(nn.Module):\n",
        "    def __init__(self, inp_c, out_c, kernel_size=4, stride=2, padding=1, use_bias=True, normalization='batch'):\n",
        "        super(DownSample, self).__init__()\n",
        "\n",
        "        self.down = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=inp_c, out_channels=out_c, kernel_size=kernel_size, stride=stride, padding=padding, bias=(not normalization) and use_bias),\n",
        "        )\n",
        "\n",
        "        if (normalization == 'batch'):\n",
        "            self.down.append(nn.BatchNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True))\n",
        "        elif (normalization == 'instance'):\n",
        "            self.down.append(nn.InstanceNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=False, track_running_stats=False))\n",
        "\n",
        "        self.down.append(nn.LeakyReLU(negative_slope=0.2, inplace=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    # def __init__(self, inp_c, out_c, kernel_size=4, stride=1, padding=0, use_bias = True, normalization='batch', apply_dropout=False, dropout_rate=0.5):\n",
        "    def __init__(self, inp_c, out_c, kernel_size=4, stride=2, padding=1, use_bias = True, normalization='batch', apply_dropout=False, dropout_rate=0.5):\n",
        "        super(UpSample, self).__init__()\n",
        "\n",
        "        # self.up = nn.Sequential(\n",
        "        #     nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "        #     nn.ZeroPad2d((2,1,2,1)),\n",
        "        #     nn.Conv2d(in_channels=inp_c, out_channels=out_c, kernel_size=kernel_size, stride=stride, padding=padding, bias=(not normalization) and use_bias),\n",
        "        # )\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=inp_c, out_channels=out_c, kernel_size=kernel_size, stride=stride, padding=padding, bias=(not normalization) and use_bias),\n",
        "        )\n",
        "\n",
        "        if (normalization == 'batch'):\n",
        "            self.up.append(nn.BatchNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True))\n",
        "        elif (normalization == 'isinstance'):\n",
        "            self.up.append(nn.InstanceNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=False, track_running_stats=False))\n",
        "\n",
        "        if apply_dropout:\n",
        "            self.up.append(nn.Dropout(p=dropout_rate, inplace=False))\n",
        "\n",
        "        self.up.append(nn.ReLU(inplace=False))\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.down_stack = nn.ModuleList([\n",
        "            DownSample(inp_c=in_channels, out_c=64, normalization=None),\n",
        "            DownSample(inp_c= 64, out_c=128),\n",
        "            DownSample(inp_c=128, out_c=256),\n",
        "            DownSample(inp_c=256, out_c=512),\n",
        "            DownSample(inp_c=512, out_c=512),\n",
        "            DownSample(inp_c=512, out_c=512),\n",
        "            DownSample(inp_c=512, out_c=512),\n",
        "            DownSample(inp_c=512, out_c=512, normalization=None),\n",
        "            ])\n",
        "\n",
        "        self.up_stack = nn.ModuleList([\n",
        "            UpSample(inp_c= 512, out_c=512), # removed dropout layers\n",
        "            UpSample(inp_c=1024, out_c=512),\n",
        "            UpSample(inp_c=1024, out_c=512),\n",
        "            UpSample(inp_c=1024, out_c=512),\n",
        "            UpSample(inp_c=1024, out_c=256),\n",
        "            UpSample(inp_c= 512, out_c=128),\n",
        "            UpSample(inp_c= 256, out_c= 64),\n",
        "            ])\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "            nn.ZeroPad2d((2,1,2,1)),\n",
        "            nn.Conv2d(in_channels=128, out_channels=out_channels, kernel_size=4, stride=1, padding=0, bias=True),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        skips = []\n",
        "        for layer in self.down_stack:\n",
        "            x = layer(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        skips.pop()\n",
        "        skips = skips[::-1]\n",
        "\n",
        "        for layer, skip in zip(self.up_stack, skips):\n",
        "            x = layer(x, skip)\n",
        "\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=4):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            DownSample(inp_c=in_channels, out_c=64, normalization=None),\n",
        "            DownSample(inp_c= 64, out_c=128),\n",
        "            DownSample(inp_c=128, out_c=256),\n",
        "            DownSample(inp_c=256, out_c=512, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=True),\n",
        "        )\n",
        "\n",
        "        # self.model = nn.Sequential(\n",
        "        #     DownSample(inp_c=in_channels, out_c=64, normalization=None),\n",
        "        #     DownSample(inp_c= 64, out_c=128),\n",
        "        #     DownSample(inp_c=128, out_c=256, padding=0),\n",
        "        #     nn.Conv2d(in_channels=256, out_channels=1, kernel_size=4, stride=1, padding=1, bias=True),\n",
        "        # )\n",
        "\n",
        "    def forward(self, sar_img, col_img):\n",
        "        x = torch.cat([sar_img, col_img], dim=1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "63b5jESIYA5_"
      },
      "outputs": [],
      "source": [
        "generator = Generator(in_channels=1, out_channels=3).to(device=DEVICE)\n",
        "discriminator = Discriminator(in_channels=4).to(device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xiJ4a5egNiGY"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeVufF6KYA6B",
        "outputId": "079e4c0e-a1f6-4c37-8972-039232989b3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (down_stack): ModuleList(\n",
              "    (0): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "        (1): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (1): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (2): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (3): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (4-6): 3 x DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (7): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "        (1): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up_stack): ModuleList(\n",
              "    (0): UpSample(\n",
              "      (up): Sequential(\n",
              "        (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (1-3): 3 x UpSample(\n",
              "      (up): Sequential(\n",
              "        (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (4): UpSample(\n",
              "      (up): Sequential(\n",
              "        (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (5): UpSample(\n",
              "      (up): Sequential(\n",
              "        (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (6): UpSample(\n",
              "      (up): Sequential(\n",
              "        (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final): Sequential(\n",
              "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "    (1): ZeroPad2d((2, 1, 2, 1))\n",
              "    (2): Conv2d(128, 3, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (3): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWIplc3gYA6C",
        "outputId": "ef0f2f43-dffc-49de-d288-da15c951d060"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "        (1): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (1): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (2): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (3): DownSample(\n",
              "      (down): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.2)\n",
              "      )\n",
              "    )\n",
              "    (4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "discriminator.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onBcQlxOYA6D",
        "outputId": "5b427e19-b43d-4a9e-e0c8-ae32badca17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           1,088\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "        DownSample-3         [-1, 64, 128, 128]               0\n",
            "            Conv2d-4          [-1, 128, 64, 64]         131,072\n",
            "       BatchNorm2d-5          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-6          [-1, 128, 64, 64]               0\n",
            "        DownSample-7          [-1, 128, 64, 64]               0\n",
            "            Conv2d-8          [-1, 256, 32, 32]         524,288\n",
            "       BatchNorm2d-9          [-1, 256, 32, 32]             512\n",
            "        LeakyReLU-10          [-1, 256, 32, 32]               0\n",
            "       DownSample-11          [-1, 256, 32, 32]               0\n",
            "           Conv2d-12          [-1, 512, 16, 16]       2,097,152\n",
            "      BatchNorm2d-13          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-14          [-1, 512, 16, 16]               0\n",
            "       DownSample-15          [-1, 512, 16, 16]               0\n",
            "           Conv2d-16            [-1, 512, 8, 8]       4,194,304\n",
            "      BatchNorm2d-17            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-18            [-1, 512, 8, 8]               0\n",
            "       DownSample-19            [-1, 512, 8, 8]               0\n",
            "           Conv2d-20            [-1, 512, 4, 4]       4,194,304\n",
            "      BatchNorm2d-21            [-1, 512, 4, 4]           1,024\n",
            "        LeakyReLU-22            [-1, 512, 4, 4]               0\n",
            "       DownSample-23            [-1, 512, 4, 4]               0\n",
            "           Conv2d-24            [-1, 512, 2, 2]       4,194,304\n",
            "      BatchNorm2d-25            [-1, 512, 2, 2]           1,024\n",
            "        LeakyReLU-26            [-1, 512, 2, 2]               0\n",
            "       DownSample-27            [-1, 512, 2, 2]               0\n",
            "           Conv2d-28            [-1, 512, 1, 1]       4,194,816\n",
            "        LeakyReLU-29            [-1, 512, 1, 1]               0\n",
            "       DownSample-30            [-1, 512, 1, 1]               0\n",
            "         Upsample-31            [-1, 512, 2, 2]               0\n",
            "        ZeroPad2d-32            [-1, 512, 5, 5]               0\n",
            "           Conv2d-33            [-1, 512, 2, 2]       4,194,304\n",
            "      BatchNorm2d-34            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-35            [-1, 512, 2, 2]               0\n",
            "         UpSample-36           [-1, 1024, 2, 2]               0\n",
            "         Upsample-37           [-1, 1024, 4, 4]               0\n",
            "        ZeroPad2d-38           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-39            [-1, 512, 4, 4]       8,388,608\n",
            "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-41            [-1, 512, 4, 4]               0\n",
            "         UpSample-42           [-1, 1024, 4, 4]               0\n",
            "         Upsample-43           [-1, 1024, 8, 8]               0\n",
            "        ZeroPad2d-44         [-1, 1024, 11, 11]               0\n",
            "           Conv2d-45            [-1, 512, 8, 8]       8,388,608\n",
            "      BatchNorm2d-46            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-47            [-1, 512, 8, 8]               0\n",
            "         UpSample-48           [-1, 1024, 8, 8]               0\n",
            "         Upsample-49         [-1, 1024, 16, 16]               0\n",
            "        ZeroPad2d-50         [-1, 1024, 19, 19]               0\n",
            "           Conv2d-51          [-1, 512, 16, 16]       8,388,608\n",
            "      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-53          [-1, 512, 16, 16]               0\n",
            "         UpSample-54         [-1, 1024, 16, 16]               0\n",
            "         Upsample-55         [-1, 1024, 32, 32]               0\n",
            "        ZeroPad2d-56         [-1, 1024, 35, 35]               0\n",
            "           Conv2d-57          [-1, 256, 32, 32]       4,194,304\n",
            "      BatchNorm2d-58          [-1, 256, 32, 32]             512\n",
            "             ReLU-59          [-1, 256, 32, 32]               0\n",
            "         UpSample-60          [-1, 512, 32, 32]               0\n",
            "         Upsample-61          [-1, 512, 64, 64]               0\n",
            "        ZeroPad2d-62          [-1, 512, 67, 67]               0\n",
            "           Conv2d-63          [-1, 128, 64, 64]       1,048,576\n",
            "      BatchNorm2d-64          [-1, 128, 64, 64]             256\n",
            "             ReLU-65          [-1, 128, 64, 64]               0\n",
            "         UpSample-66          [-1, 256, 64, 64]               0\n",
            "         Upsample-67        [-1, 256, 128, 128]               0\n",
            "        ZeroPad2d-68        [-1, 256, 131, 131]               0\n",
            "           Conv2d-69         [-1, 64, 128, 128]         262,144\n",
            "      BatchNorm2d-70         [-1, 64, 128, 128]             128\n",
            "             ReLU-71         [-1, 64, 128, 128]               0\n",
            "         UpSample-72        [-1, 128, 128, 128]               0\n",
            "         Upsample-73        [-1, 128, 256, 256]               0\n",
            "        ZeroPad2d-74        [-1, 128, 259, 259]               0\n",
            "           Conv2d-75          [-1, 3, 256, 256]           6,147\n",
            "             Tanh-76          [-1, 3, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 54,412,483\n",
            "Trainable params: 54,412,483\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 385.98\n",
            "Params size (MB): 207.57\n",
            "Estimated Total Size (MB): 593.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# from torchsummary import summary\n",
        "\n",
        "# summary(generator, input_size=(1, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuu9S76WYA6D",
        "outputId": "7f7cd8cd-a061-4c7d-cb6e-4fe8e1a484eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           4,160\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "        DownSample-3         [-1, 64, 128, 128]               0\n",
            "            Conv2d-4          [-1, 128, 64, 64]         131,072\n",
            "       BatchNorm2d-5          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-6          [-1, 128, 64, 64]               0\n",
            "        DownSample-7          [-1, 128, 64, 64]               0\n",
            "            Conv2d-8          [-1, 256, 32, 32]         524,288\n",
            "       BatchNorm2d-9          [-1, 256, 32, 32]             512\n",
            "        LeakyReLU-10          [-1, 256, 32, 32]               0\n",
            "       DownSample-11          [-1, 256, 32, 32]               0\n",
            "           Conv2d-12          [-1, 512, 31, 31]       2,097,152\n",
            "      BatchNorm2d-13          [-1, 512, 31, 31]           1,024\n",
            "        LeakyReLU-14          [-1, 512, 31, 31]               0\n",
            "       DownSample-15          [-1, 512, 31, 31]               0\n",
            "           Conv2d-16            [-1, 1, 30, 30]           8,193\n",
            "================================================================\n",
            "Total params: 2,766,657\n",
            "Trainable params: 2,766,657\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 49152.00\n",
            "Forward/backward pass size (MB): 63.02\n",
            "Params size (MB): 10.55\n",
            "Estimated Total Size (MB): 49225.58\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# summary(discriminator, input_size=[(1, 256, 256), (3, 256, 256)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA9MPw3WYA6F",
        "outputId": "7c079a96-3b8e-4e92-ad5b-9c37ae511377"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 174MB/s]\n"
          ]
        }
      ],
      "source": [
        "bce_loss = nn.BCEWithLogitsLoss().to(device=DEVICE)\n",
        "mae_loss = nn.L1Loss().to(device=DEVICE)\n",
        "\n",
        "feature_extractor = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features[:27].to(DEVICE).eval()\n",
        "for param in feature_extractor.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "def extract_features(img):\n",
        "      img = (img + 1) / 2\n",
        "      img = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))(img)\n",
        "      return feature_extractor(img)\n",
        "\n",
        "def generator_loss(generated, target, patch_fake, lambda_image=100.0, lambda_perceptual=20.0):\n",
        "    gen_f, tar_f = extract_features(generated), extract_features(target)\n",
        "\n",
        "    bce_loss_G = bce_loss(patch_fake, torch.full_like(patch_fake, fill_value=0.9, device=DEVICE))\n",
        "    img_loss_G = mae_loss(generated, target)\n",
        "    per_loss_G = mae_loss(gen_f, tar_f)\n",
        "\n",
        "    loss_G = bce_loss_G + lambda_image * img_loss_G + lambda_perceptual * per_loss_G\n",
        "\n",
        "    return loss_G, bce_loss_G.item(), img_loss_G.item(), per_loss_G.item()\n",
        "\n",
        "def discriminator_loss(patch_fake, patch_valid):\n",
        "    valid_loss_D = bce_loss(patch_valid, torch.full_like(patch_valid, fill_value=0.9, device=DEVICE))\n",
        "    fake_loss_D = bce_loss(patch_fake, torch.full_like(patch_fake, fill_value=0.1, device=DEVICE))\n",
        "\n",
        "    loss_D = 0.5 * (valid_loss_D + fake_loss_D)\n",
        "\n",
        "    return  loss_D, valid_loss_D.item(), fake_loss_D.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4gscIR5PYA6G"
      },
      "outputs": [],
      "source": [
        "optimizer_G = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2pd-zd1JYA6G"
      },
      "outputs": [],
      "source": [
        "def visualize_results(sar, fake, col):\n",
        "    \"\"\"\n",
        "    Display SAR (grayscale), FAKE (RGB), and COL (RGB) images side by side.\n",
        "\n",
        "    Args:\n",
        "        sar (torch.Tensor): Grayscale image (1, H, W), range [-1, 1].\n",
        "        fake (torch.Tensor): RGB image (3, H, W), range [-1, 1].\n",
        "        col (torch.Tensor): RGB image (3, H, W), range [-1, 1].\n",
        "    \"\"\"\n",
        "    def tensor_to_image(tensor, cmap=None):\n",
        "        tensor = (tensor.clamp(-1, 1) + 1) / 2  # Normalize [-1,1] to [0,1]\n",
        "        tensor = tensor.cpu().detach().numpy()  # Convert to NumPy\n",
        "        if tensor.shape[0] == 1:  # Grayscale (1, H, W) -> (H, W)\n",
        "            return tensor[0], \"gray\"\n",
        "        return tensor.transpose(1, 2, 0), None  # RGB (3, H, W) -> (H, W, 3)\n",
        "\n",
        "    images = [sar, fake, col]\n",
        "    titles = [\"SAR\", \"Generated\", \"Optical\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    for ax, img, title in zip(axes, images, titles):\n",
        "        img, cmap = tensor_to_image(img)\n",
        "        ax.imshow(img, cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MKht1RTFYA6H",
        "outputId": "2d3a6192-b108-4936-dbfa-c6e2ea1bea74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [001/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.751097 Loss_G: 40.942726 | Valid_D: 0.765656 Fake_D: 0.736539 | BCE_G: 0.787108 Img_G: 0.327095 Per_G: 0.372304\n",
            " Batch [050/125] | Loss_D: 0.714026 Loss_G: 40.251263 | Valid_D: 0.716103 Fake_D: 0.711949 | BCE_G: 0.787265 Img_G: 0.323352 Per_G: 0.356439\n",
            " Batch [075/125] | Loss_D: 0.645039 Loss_G: 39.539150 | Valid_D: 0.664948 Fake_D: 0.625130 | BCE_G: 0.866222 Img_G: 0.317622 Per_G: 0.345538\n",
            " Batch [100/125] | Loss_D: 0.550900 Loss_G: 38.552498 | Valid_D: 0.538051 Fake_D: 0.563748 | BCE_G: 0.959976 Img_G: 0.308306 Per_G: 0.338096\n",
            " Batch [125/125] | Loss_D: 0.562957 Loss_G: 39.643475 | Valid_D: 0.618244 Fake_D: 0.507670 | BCE_G: 1.059304 Img_G: 0.312107 Per_G: 0.368671\n",
            " ----------------------------------------------------\n",
            " Epoch [001/100] | Loss_D: 0.682885 Loss_G: 40.915904 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [002/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.551293 Loss_G: 42.277187 | Valid_D: 0.474489 Fake_D: 0.628097 | BCE_G: 0.889833 Img_G: 0.343091 Per_G: 0.353910\n",
            " Batch [050/125] | Loss_D: 0.468905 Loss_G: 38.549210 | Valid_D: 0.451956 Fake_D: 0.485854 | BCE_G: 1.137338 Img_G: 0.303563 Per_G: 0.352777\n",
            " Batch [075/125] | Loss_D: 0.459418 Loss_G: 40.858299 | Valid_D: 0.500309 Fake_D: 0.418527 | BCE_G: 1.342667 Img_G: 0.320368 Per_G: 0.373940\n",
            " Batch [100/125] | Loss_D: 0.524766 Loss_G: 44.248398 | Valid_D: 0.516212 Fake_D: 0.533320 | BCE_G: 1.093308 Img_G: 0.356826 Per_G: 0.373623\n",
            " Batch [125/125] | Loss_D: 0.428538 Loss_G: 42.849766 | Valid_D: 0.442631 Fake_D: 0.414446 | BCE_G: 1.389854 Img_G: 0.342618 Per_G: 0.359904\n",
            " ----------------------------------------------------\n",
            " Epoch [002/100] | Loss_D: 0.492200 Loss_G: 40.246453 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [003/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.405014 Loss_G: 40.757099 | Valid_D: 0.407751 Fake_D: 0.402278 | BCE_G: 1.536349 Img_G: 0.326674 Per_G: 0.327668\n",
            " Batch [050/125] | Loss_D: 0.452978 Loss_G: 39.261890 | Valid_D: 0.505917 Fake_D: 0.400038 | BCE_G: 1.363865 Img_G: 0.301920 Per_G: 0.385301\n",
            " Batch [075/125] | Loss_D: 0.434718 Loss_G: 39.917690 | Valid_D: 0.499681 Fake_D: 0.369754 | BCE_G: 1.587882 Img_G: 0.313431 Per_G: 0.349336\n",
            " Batch [100/125] | Loss_D: 0.589177 Loss_G: 40.609474 | Valid_D: 0.480814 Fake_D: 0.697539 | BCE_G: 0.825194 Img_G: 0.323936 Per_G: 0.369532\n",
            " Batch [125/125] | Loss_D: 0.382711 Loss_G: 39.746029 | Valid_D: 0.420135 Fake_D: 0.345287 | BCE_G: 1.815042 Img_G: 0.305394 Per_G: 0.369579\n",
            " ----------------------------------------------------\n",
            " Epoch [003/100] | Loss_D: 0.448556 Loss_G: 40.190338 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [004/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.415977 Loss_G: 35.774101 | Valid_D: 0.469442 Fake_D: 0.362512 | BCE_G: 1.604824 Img_G: 0.276148 Per_G: 0.327724\n",
            " Batch [050/125] | Loss_D: 0.445059 Loss_G: 36.415852 | Valid_D: 0.442136 Fake_D: 0.447981 | BCE_G: 1.222251 Img_G: 0.284440 Per_G: 0.337482\n",
            " Batch [075/125] | Loss_D: 0.348681 Loss_G: 39.219013 | Valid_D: 0.359456 Fake_D: 0.337906 | BCE_G: 2.033710 Img_G: 0.299986 Per_G: 0.359336\n",
            " Batch [100/125] | Loss_D: 0.455942 Loss_G: 42.164112 | Valid_D: 0.364118 Fake_D: 0.547765 | BCE_G: 1.098598 Img_G: 0.339269 Per_G: 0.356930\n",
            " Batch [125/125] | Loss_D: 0.371556 Loss_G: 45.826908 | Valid_D: 0.385578 Fake_D: 0.357533 | BCE_G: 1.750028 Img_G: 0.371262 Per_G: 0.347533\n",
            " ----------------------------------------------------\n",
            " Epoch [004/100] | Loss_D: 0.422972 Loss_G: 40.082388 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [005/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.361358 Loss_G: 38.365288 | Valid_D: 0.378503 Fake_D: 0.344213 | BCE_G: 1.828218 Img_G: 0.290058 Per_G: 0.376561\n",
            " Batch [050/125] | Loss_D: 0.586660 Loss_G: 41.345081 | Valid_D: 0.415571 Fake_D: 0.757750 | BCE_G: 0.828981 Img_G: 0.332742 Per_G: 0.362093\n",
            " Batch [075/125] | Loss_D: 0.370539 Loss_G: 40.465233 | Valid_D: 0.384230 Fake_D: 0.356848 | BCE_G: 1.719761 Img_G: 0.318014 Per_G: 0.347206\n",
            " Batch [100/125] | Loss_D: 0.383992 Loss_G: 43.896976 | Valid_D: 0.379519 Fake_D: 0.388466 | BCE_G: 1.521374 Img_G: 0.352899 Per_G: 0.354285\n",
            " Batch [125/125] | Loss_D: 0.396325 Loss_G: 42.019394 | Valid_D: 0.394828 Fake_D: 0.397822 | BCE_G: 1.426525 Img_G: 0.335513 Per_G: 0.352080\n",
            " ----------------------------------------------------\n",
            " Epoch [005/100] | Loss_D: 0.422177 Loss_G: 39.954561 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [006/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.398611 Loss_G: 41.894154 | Valid_D: 0.346943 Fake_D: 0.450279 | BCE_G: 1.297718 Img_G: 0.339239 Per_G: 0.333627\n",
            " Batch [050/125] | Loss_D: 0.436078 Loss_G: 35.505203 | Valid_D: 0.455305 Fake_D: 0.416852 | BCE_G: 1.337700 Img_G: 0.271508 Per_G: 0.350837\n",
            " Batch [075/125] | Loss_D: 0.367555 Loss_G: 38.595188 | Valid_D: 0.375940 Fake_D: 0.359170 | BCE_G: 1.667480 Img_G: 0.296370 Per_G: 0.364533\n",
            " Batch [100/125] | Loss_D: 0.346989 Loss_G: 46.803097 | Valid_D: 0.349736 Fake_D: 0.344241 | BCE_G: 1.945460 Img_G: 0.381014 Per_G: 0.337809\n",
            " Batch [125/125] | Loss_D: 0.363841 Loss_G: 39.141514 | Valid_D: 0.381647 Fake_D: 0.346034 | BCE_G: 1.917919 Img_G: 0.302707 Per_G: 0.347646\n",
            " ----------------------------------------------------\n",
            " Epoch [006/100] | Loss_D: 0.404938 Loss_G: 39.284488 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [007/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.567632 Loss_G: 37.709675 | Valid_D: 0.437313 Fake_D: 0.697952 | BCE_G: 1.063137 Img_G: 0.297238 Per_G: 0.346139\n",
            " Batch [050/125] | Loss_D: 0.347271 Loss_G: 42.245216 | Valid_D: 0.351629 Fake_D: 0.342913 | BCE_G: 1.942489 Img_G: 0.335001 Per_G: 0.340132\n",
            " Batch [075/125] | Loss_D: 0.348658 Loss_G: 42.467564 | Valid_D: 0.340120 Fake_D: 0.357195 | BCE_G: 1.777058 Img_G: 0.340585 Per_G: 0.331599\n",
            " Batch [100/125] | Loss_D: 0.457144 Loss_G: 36.269703 | Valid_D: 0.436435 Fake_D: 0.477852 | BCE_G: 1.200255 Img_G: 0.279581 Per_G: 0.355568\n",
            " Batch [125/125] | Loss_D: 0.411396 Loss_G: 39.725414 | Valid_D: 0.380772 Fake_D: 0.442021 | BCE_G: 1.413096 Img_G: 0.313150 Per_G: 0.349864\n",
            " ----------------------------------------------------\n",
            " Epoch [007/100] | Loss_D: 0.406263 Loss_G: 38.788547 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [008/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.399609 Loss_G: 38.873505 | Valid_D: 0.339185 Fake_D: 0.460033 | BCE_G: 1.331802 Img_G: 0.302118 Per_G: 0.366495\n",
            " Batch [050/125] | Loss_D: 0.443500 Loss_G: 38.944904 | Valid_D: 0.545143 Fake_D: 0.341858 | BCE_G: 1.944875 Img_G: 0.301350 Per_G: 0.343252\n",
            " Batch [075/125] | Loss_D: 0.473681 Loss_G: 37.272602 | Valid_D: 0.585272 Fake_D: 0.362090 | BCE_G: 1.584485 Img_G: 0.290907 Per_G: 0.329872\n",
            " Batch [100/125] | Loss_D: 0.400085 Loss_G: 37.727459 | Valid_D: 0.450146 Fake_D: 0.350023 | BCE_G: 1.742222 Img_G: 0.283700 Per_G: 0.380760\n",
            " Batch [125/125] | Loss_D: 0.378579 Loss_G: 38.239098 | Valid_D: 0.389468 Fake_D: 0.367690 | BCE_G: 1.604476 Img_G: 0.292771 Per_G: 0.367878\n",
            " ----------------------------------------------------\n",
            " Epoch [008/100] | Loss_D: 0.404008 Loss_G: 38.278610 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [009/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.347978 Loss_G: 38.738857 | Valid_D: 0.357103 Fake_D: 0.338853 | BCE_G: 1.983939 Img_G: 0.296459 Per_G: 0.355452\n",
            " Batch [050/125] | Loss_D: 0.430648 Loss_G: 38.379860 | Valid_D: 0.479426 Fake_D: 0.381869 | BCE_G: 1.500958 Img_G: 0.294676 Per_G: 0.370566\n",
            " Batch [075/125] | Loss_D: 0.362362 Loss_G: 39.098782 | Valid_D: 0.354769 Fake_D: 0.369955 | BCE_G: 1.544038 Img_G: 0.300211 Per_G: 0.376684\n",
            " Batch [100/125] | Loss_D: 0.435862 Loss_G: 40.016369 | Valid_D: 0.358114 Fake_D: 0.513610 | BCE_G: 1.175755 Img_G: 0.315786 Per_G: 0.363101\n",
            " Batch [125/125] | Loss_D: 0.373606 Loss_G: 37.944424 | Valid_D: 0.340464 Fake_D: 0.406749 | BCE_G: 1.487408 Img_G: 0.298521 Per_G: 0.330244\n",
            " ----------------------------------------------------\n",
            " Epoch [009/100] | Loss_D: 0.397308 Loss_G: 37.805917 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [010/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.357188 Loss_G: 38.155212 | Valid_D: 0.363511 Fake_D: 0.350866 | BCE_G: 1.768627 Img_G: 0.290036 Per_G: 0.369148\n",
            " Batch [050/125] | Loss_D: 0.359605 Loss_G: 37.566017 | Valid_D: 0.369229 Fake_D: 0.349980 | BCE_G: 1.798794 Img_G: 0.289399 Per_G: 0.341366\n",
            " Batch [075/125] | Loss_D: 0.389975 Loss_G: 37.211662 | Valid_D: 0.406892 Fake_D: 0.373058 | BCE_G: 1.523597 Img_G: 0.284231 Per_G: 0.363248\n",
            " Batch [100/125] | Loss_D: 0.509167 Loss_G: 37.753231 | Valid_D: 0.507721 Fake_D: 0.510612 | BCE_G: 1.144925 Img_G: 0.295666 Per_G: 0.352087\n",
            " Batch [125/125] | Loss_D: 0.526561 Loss_G: 36.197777 | Valid_D: 0.655479 Fake_D: 0.397643 | BCE_G: 1.322788 Img_G: 0.275096 Per_G: 0.368268\n",
            " ----------------------------------------------------\n",
            " Epoch [010/100] | Loss_D: 0.408161 Loss_G: 37.301815 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [011/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.397460 Loss_G: 34.797337 | Valid_D: 0.348866 Fake_D: 0.446055 | BCE_G: 1.376258 Img_G: 0.263621 Per_G: 0.352950\n",
            " Batch [050/125] | Loss_D: 0.397145 Loss_G: 38.226460 | Valid_D: 0.342071 Fake_D: 0.452220 | BCE_G: 1.394667 Img_G: 0.294528 Per_G: 0.368949\n",
            " Batch [075/125] | Loss_D: 0.476771 Loss_G: 34.396767 | Valid_D: 0.346543 Fake_D: 0.606999 | BCE_G: 1.107632 Img_G: 0.262490 Per_G: 0.352007\n",
            " Batch [100/125] | Loss_D: 0.396871 Loss_G: 36.598984 | Valid_D: 0.393584 Fake_D: 0.400158 | BCE_G: 1.423446 Img_G: 0.281484 Per_G: 0.351356\n",
            " Batch [125/125] | Loss_D: 0.368301 Loss_G: 37.411209 | Valid_D: 0.369200 Fake_D: 0.367401 | BCE_G: 1.613026 Img_G: 0.281863 Per_G: 0.380593\n",
            " ----------------------------------------------------\n",
            " Epoch [011/100] | Loss_D: 0.412668 Loss_G: 36.792209 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [012/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.418822 Loss_G: 38.325146 | Valid_D: 0.358903 Fake_D: 0.478742 | BCE_G: 1.413242 Img_G: 0.295143 Per_G: 0.369879\n",
            " Batch [050/125] | Loss_D: 0.352634 Loss_G: 37.619587 | Valid_D: 0.339500 Fake_D: 0.365769 | BCE_G: 1.628007 Img_G: 0.285656 Per_G: 0.371297\n",
            " Batch [075/125] | Loss_D: 0.480152 Loss_G: 38.033508 | Valid_D: 0.362707 Fake_D: 0.597598 | BCE_G: 1.177584 Img_G: 0.290253 Per_G: 0.391529\n",
            " Batch [100/125] | Loss_D: 0.452207 Loss_G: 36.768166 | Valid_D: 0.398828 Fake_D: 0.505586 | BCE_G: 1.203668 Img_G: 0.279785 Per_G: 0.379301\n",
            " Batch [125/125] | Loss_D: 0.369043 Loss_G: 34.707684 | Valid_D: 0.373398 Fake_D: 0.364688 | BCE_G: 1.663765 Img_G: 0.258578 Per_G: 0.359305\n",
            " ----------------------------------------------------\n",
            " Epoch [012/100] | Loss_D: 0.413735 Loss_G: 36.289841 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [013/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.382197 Loss_G: 35.953621 | Valid_D: 0.414419 Fake_D: 0.349976 | BCE_G: 1.735504 Img_G: 0.266203 Per_G: 0.379892\n",
            " Batch [050/125] | Loss_D: 0.356064 Loss_G: 34.998623 | Valid_D: 0.366866 Fake_D: 0.345262 | BCE_G: 1.802724 Img_G: 0.260159 Per_G: 0.359002\n",
            " Batch [075/125] | Loss_D: 0.485994 Loss_G: 35.309414 | Valid_D: 0.467921 Fake_D: 0.504066 | BCE_G: 1.143073 Img_G: 0.271054 Per_G: 0.353046\n",
            " Batch [100/125] | Loss_D: 0.461995 Loss_G: 35.378807 | Valid_D: 0.475078 Fake_D: 0.448912 | BCE_G: 1.281220 Img_G: 0.271137 Per_G: 0.349193\n",
            " Batch [125/125] | Loss_D: 0.363915 Loss_G: 35.783951 | Valid_D: 0.366168 Fake_D: 0.361662 | BCE_G: 1.641406 Img_G: 0.271662 Per_G: 0.348816\n",
            " ----------------------------------------------------\n",
            " Epoch [013/100] | Loss_D: 0.409013 Loss_G: 35.686807 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [014/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.486190 Loss_G: 35.178822 | Valid_D: 0.396518 Fake_D: 0.575862 | BCE_G: 1.069097 Img_G: 0.266204 Per_G: 0.374465\n",
            " Batch [050/125] | Loss_D: 0.468954 Loss_G: 33.578636 | Valid_D: 0.518581 Fake_D: 0.419327 | BCE_G: 1.304048 Img_G: 0.250392 Per_G: 0.361768\n",
            " Batch [075/125] | Loss_D: 0.411837 Loss_G: 36.683304 | Valid_D: 0.363580 Fake_D: 0.460093 | BCE_G: 1.229070 Img_G: 0.279565 Per_G: 0.374888\n",
            " Batch [100/125] | Loss_D: 0.341038 Loss_G: 36.680260 | Valid_D: 0.336758 Fake_D: 0.345318 | BCE_G: 1.760011 Img_G: 0.277350 Per_G: 0.359264\n",
            " Batch [125/125] | Loss_D: 0.426633 Loss_G: 34.133057 | Valid_D: 0.443815 Fake_D: 0.409450 | BCE_G: 1.383792 Img_G: 0.259875 Per_G: 0.338089\n",
            " ----------------------------------------------------\n",
            " Epoch [014/100] | Loss_D: 0.418010 Loss_G: 34.980886 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [015/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.396850 Loss_G: 34.131203 | Valid_D: 0.455861 Fake_D: 0.337839 | BCE_G: 2.213917 Img_G: 0.250379 Per_G: 0.343969\n",
            " Batch [050/125] | Loss_D: 0.412315 Loss_G: 32.355457 | Valid_D: 0.475550 Fake_D: 0.349080 | BCE_G: 1.735942 Img_G: 0.236280 Per_G: 0.349576\n",
            " Batch [075/125] | Loss_D: 0.528108 Loss_G: 33.447151 | Valid_D: 0.589828 Fake_D: 0.466387 | BCE_G: 1.236532 Img_G: 0.248692 Per_G: 0.367070\n",
            " Batch [100/125] | Loss_D: 0.339342 Loss_G: 34.397800 | Valid_D: 0.341107 Fake_D: 0.337577 | BCE_G: 1.989121 Img_G: 0.251518 Per_G: 0.362846\n",
            " Batch [125/125] | Loss_D: 0.359815 Loss_G: 37.278919 | Valid_D: 0.343894 Fake_D: 0.375735 | BCE_G: 1.859233 Img_G: 0.278743 Per_G: 0.377271\n",
            " ----------------------------------------------------\n",
            " Epoch [015/100] | Loss_D: 0.418289 Loss_G: 34.418420 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [016/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.470965 Loss_G: 33.244831 | Valid_D: 0.362790 Fake_D: 0.579140 | BCE_G: 1.098753 Img_G: 0.247369 Per_G: 0.370461\n",
            " Batch [050/125] | Loss_D: 0.353710 Loss_G: 34.906597 | Valid_D: 0.340582 Fake_D: 0.366837 | BCE_G: 1.584746 Img_G: 0.262229 Per_G: 0.354948\n",
            " Batch [075/125] | Loss_D: 0.596551 Loss_G: 34.788692 | Valid_D: 0.557646 Fake_D: 0.635457 | BCE_G: 0.934227 Img_G: 0.266657 Per_G: 0.359439\n",
            " Batch [100/125] | Loss_D: 0.411345 Loss_G: 32.784809 | Valid_D: 0.397843 Fake_D: 0.424847 | BCE_G: 1.321206 Img_G: 0.245189 Per_G: 0.347238\n",
            " Batch [125/125] | Loss_D: 0.513101 Loss_G: 31.700371 | Valid_D: 0.686274 Fake_D: 0.339929 | BCE_G: 1.658804 Img_G: 0.233153 Per_G: 0.336314\n",
            " ----------------------------------------------------\n",
            " Epoch [016/100] | Loss_D: 0.430339 Loss_G: 33.786556 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [017/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.396505 Loss_G: 32.630089 | Valid_D: 0.338067 Fake_D: 0.454943 | BCE_G: 1.481478 Img_G: 0.243280 Per_G: 0.341030\n",
            " Batch [050/125] | Loss_D: 0.576602 Loss_G: 32.452869 | Valid_D: 0.467295 Fake_D: 0.685909 | BCE_G: 1.087707 Img_G: 0.246689 Per_G: 0.334813\n",
            " Batch [075/125] | Loss_D: 0.686574 Loss_G: 32.419678 | Valid_D: 1.034181 Fake_D: 0.338967 | BCE_G: 1.876382 Img_G: 0.237683 Per_G: 0.338750\n",
            " Batch [100/125] | Loss_D: 0.536059 Loss_G: 32.955635 | Valid_D: 0.396280 Fake_D: 0.675838 | BCE_G: 0.887969 Img_G: 0.246411 Per_G: 0.371331\n",
            " Batch [125/125] | Loss_D: 0.434189 Loss_G: 32.546093 | Valid_D: 0.524769 Fake_D: 0.343609 | BCE_G: 1.595007 Img_G: 0.238730 Per_G: 0.353902\n",
            " ----------------------------------------------------\n",
            " Epoch [017/100] | Loss_D: 0.434446 Loss_G: 33.136685 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [018/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.344897 Loss_G: 33.636116 | Valid_D: 0.340249 Fake_D: 0.349546 | BCE_G: 1.859514 Img_G: 0.245948 Per_G: 0.359092\n",
            " Batch [050/125] | Loss_D: 0.429260 Loss_G: 32.388615 | Valid_D: 0.516140 Fake_D: 0.342381 | BCE_G: 1.783484 Img_G: 0.231910 Per_G: 0.370708\n",
            " Batch [075/125] | Loss_D: 0.450813 Loss_G: 31.570318 | Valid_D: 0.393896 Fake_D: 0.507730 | BCE_G: 1.085166 Img_G: 0.231144 Per_G: 0.368539\n",
            " Batch [100/125] | Loss_D: 0.471071 Loss_G: 33.449150 | Valid_D: 0.539281 Fake_D: 0.402862 | BCE_G: 1.256748 Img_G: 0.247786 Per_G: 0.370688\n",
            " Batch [125/125] | Loss_D: 0.628975 Loss_G: 30.612703 | Valid_D: 0.918340 Fake_D: 0.339610 | BCE_G: 1.671511 Img_G: 0.217167 Per_G: 0.361225\n",
            " ----------------------------------------------------\n",
            " Epoch [018/100] | Loss_D: 0.438697 Loss_G: 32.694437 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [019/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.525647 Loss_G: 34.449966 | Valid_D: 0.346001 Fake_D: 0.705292 | BCE_G: 1.026996 Img_G: 0.259780 Per_G: 0.372248\n",
            " Batch [050/125] | Loss_D: 0.560227 Loss_G: 32.139030 | Valid_D: 0.769435 Fake_D: 0.351019 | BCE_G: 1.537022 Img_G: 0.235078 Per_G: 0.354712\n",
            " Batch [075/125] | Loss_D: 0.596976 Loss_G: 30.713360 | Valid_D: 0.857154 Fake_D: 0.336798 | BCE_G: 1.657097 Img_G: 0.219683 Per_G: 0.354400\n",
            " Batch [100/125] | Loss_D: 0.441817 Loss_G: 30.504025 | Valid_D: 0.440265 Fake_D: 0.443370 | BCE_G: 1.219733 Img_G: 0.219769 Per_G: 0.365369\n",
            " Batch [125/125] | Loss_D: 0.361600 Loss_G: 34.029930 | Valid_D: 0.378603 Fake_D: 0.344597 | BCE_G: 1.794518 Img_G: 0.247287 Per_G: 0.375334\n",
            " ----------------------------------------------------\n",
            " Epoch [019/100] | Loss_D: 0.437334 Loss_G: 31.926441 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [020/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.466964 Loss_G: 32.007080 | Valid_D: 0.491077 Fake_D: 0.442851 | BCE_G: 1.231326 Img_G: 0.233644 Per_G: 0.370570\n",
            " Batch [050/125] | Loss_D: 0.509677 Loss_G: 29.535385 | Valid_D: 0.679525 Fake_D: 0.339830 | BCE_G: 1.419990 Img_G: 0.211561 Per_G: 0.347963\n",
            " Batch [075/125] | Loss_D: 0.352575 Loss_G: 32.189545 | Valid_D: 0.352434 Fake_D: 0.352717 | BCE_G: 1.746043 Img_G: 0.232083 Per_G: 0.361762\n",
            " Batch [100/125] | Loss_D: 0.431597 Loss_G: 32.792404 | Valid_D: 0.517743 Fake_D: 0.345450 | BCE_G: 1.739292 Img_G: 0.237989 Per_G: 0.362711\n",
            " Batch [125/125] | Loss_D: 0.514301 Loss_G: 31.342995 | Valid_D: 0.682504 Fake_D: 0.346098 | BCE_G: 1.816722 Img_G: 0.226270 Per_G: 0.344964\n",
            " ----------------------------------------------------\n",
            " Epoch [020/100] | Loss_D: 0.458591 Loss_G: 31.336471 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [021/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.404098 Loss_G: 28.140104 | Valid_D: 0.415805 Fake_D: 0.392392 | BCE_G: 1.429037 Img_G: 0.199557 Per_G: 0.337770\n",
            " Batch [050/125] | Loss_D: 0.385602 Loss_G: 32.578667 | Valid_D: 0.338922 Fake_D: 0.432283 | BCE_G: 1.356788 Img_G: 0.231857 Per_G: 0.401810\n",
            " Batch [075/125] | Loss_D: 0.480106 Loss_G: 33.290298 | Valid_D: 0.540922 Fake_D: 0.419290 | BCE_G: 1.107282 Img_G: 0.250817 Per_G: 0.355066\n",
            " Batch [100/125] | Loss_D: 0.359547 Loss_G: 31.647034 | Valid_D: 0.380515 Fake_D: 0.338579 | BCE_G: 1.915863 Img_G: 0.221839 Per_G: 0.377365\n",
            " Batch [125/125] | Loss_D: 0.469907 Loss_G: 30.088520 | Valid_D: 0.387151 Fake_D: 0.552663 | BCE_G: 1.077236 Img_G: 0.216330 Per_G: 0.368916\n",
            " ----------------------------------------------------\n",
            " Epoch [021/100] | Loss_D: 0.455176 Loss_G: 31.090256 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [022/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.433706 Loss_G: 30.332195 | Valid_D: 0.382452 Fake_D: 0.484960 | BCE_G: 1.209541 Img_G: 0.219159 Per_G: 0.360337\n",
            " Batch [050/125] | Loss_D: 0.423664 Loss_G: 30.102329 | Valid_D: 0.409274 Fake_D: 0.438055 | BCE_G: 1.286395 Img_G: 0.215152 Per_G: 0.365039\n",
            " Batch [075/125] | Loss_D: 0.451597 Loss_G: 30.074654 | Valid_D: 0.431680 Fake_D: 0.471514 | BCE_G: 1.104788 Img_G: 0.210905 Per_G: 0.393967\n",
            " Batch [100/125] | Loss_D: 0.651302 Loss_G: 29.998789 | Valid_D: 0.431155 Fake_D: 0.871450 | BCE_G: 0.795255 Img_G: 0.222950 Per_G: 0.345425\n",
            " Batch [125/125] | Loss_D: 0.398197 Loss_G: 31.124119 | Valid_D: 0.403143 Fake_D: 0.393252 | BCE_G: 1.502317 Img_G: 0.223876 Per_G: 0.361711\n",
            " ----------------------------------------------------\n",
            " Epoch [022/100] | Loss_D: 0.467064 Loss_G: 30.355810 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [023/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.434315 Loss_G: 30.212061 | Valid_D: 0.348798 Fake_D: 0.519832 | BCE_G: 1.110565 Img_G: 0.217545 Per_G: 0.367348\n",
            " Batch [050/125] | Loss_D: 0.427875 Loss_G: 30.669037 | Valid_D: 0.463901 Fake_D: 0.391848 | BCE_G: 1.338493 Img_G: 0.218355 Per_G: 0.374752\n",
            " Batch [075/125] | Loss_D: 0.399525 Loss_G: 29.669678 | Valid_D: 0.418226 Fake_D: 0.380824 | BCE_G: 1.364171 Img_G: 0.213110 Per_G: 0.349726\n",
            " Batch [100/125] | Loss_D: 0.455314 Loss_G: 29.640701 | Valid_D: 0.345554 Fake_D: 0.565075 | BCE_G: 1.154880 Img_G: 0.215186 Per_G: 0.348359\n",
            " Batch [125/125] | Loss_D: 0.409943 Loss_G: 29.791037 | Valid_D: 0.343046 Fake_D: 0.476840 | BCE_G: 1.237211 Img_G: 0.213339 Per_G: 0.360997\n",
            " ----------------------------------------------------\n",
            " Epoch [023/100] | Loss_D: 0.478944 Loss_G: 29.987000 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [024/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.414984 Loss_G: 28.868452 | Valid_D: 0.374200 Fake_D: 0.455768 | BCE_G: 1.328931 Img_G: 0.200495 Per_G: 0.374501\n",
            " Batch [050/125] | Loss_D: 0.436481 Loss_G: 28.517992 | Valid_D: 0.394530 Fake_D: 0.478432 | BCE_G: 1.119485 Img_G: 0.201592 Per_G: 0.361967\n",
            " Batch [075/125] | Loss_D: 0.443820 Loss_G: 28.942652 | Valid_D: 0.399395 Fake_D: 0.488244 | BCE_G: 1.142821 Img_G: 0.204398 Per_G: 0.368003\n",
            " Batch [100/125] | Loss_D: 0.374912 Loss_G: 28.764645 | Valid_D: 0.386764 Fake_D: 0.363059 | BCE_G: 1.558606 Img_G: 0.196314 Per_G: 0.378731\n",
            " Batch [125/125] | Loss_D: 0.444393 Loss_G: 27.884691 | Valid_D: 0.421403 Fake_D: 0.467383 | BCE_G: 1.077001 Img_G: 0.196370 Per_G: 0.358535\n",
            " ----------------------------------------------------\n",
            " Epoch [024/100] | Loss_D: 0.477769 Loss_G: 29.574068 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [025/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.453292 Loss_G: 28.882601 | Valid_D: 0.338866 Fake_D: 0.567718 | BCE_G: 1.180805 Img_G: 0.204346 Per_G: 0.363360\n",
            " Batch [050/125] | Loss_D: 0.413690 Loss_G: 28.749731 | Valid_D: 0.414420 Fake_D: 0.412959 | BCE_G: 1.344797 Img_G: 0.202004 Per_G: 0.360225\n",
            " Batch [075/125] | Loss_D: 0.343492 Loss_G: 31.091656 | Valid_D: 0.335319 Fake_D: 0.351665 | BCE_G: 1.865162 Img_G: 0.216419 Per_G: 0.379229\n",
            " Batch [100/125] | Loss_D: 0.651215 Loss_G: 29.436562 | Valid_D: 0.622316 Fake_D: 0.680114 | BCE_G: 0.798540 Img_G: 0.217610 Per_G: 0.343851\n",
            " Batch [125/125] | Loss_D: 0.446892 Loss_G: 30.100161 | Valid_D: 0.389883 Fake_D: 0.503901 | BCE_G: 1.238736 Img_G: 0.214269 Per_G: 0.371725\n",
            " ----------------------------------------------------\n",
            " Epoch [025/100] | Loss_D: 0.479229 Loss_G: 29.032519 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [026/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.360828 Loss_G: 31.304134 | Valid_D: 0.336497 Fake_D: 0.385158 | BCE_G: 1.485022 Img_G: 0.225104 Per_G: 0.365435\n",
            " Batch [050/125] | Loss_D: 0.417097 Loss_G: 29.359915 | Valid_D: 0.463951 Fake_D: 0.370244 | BCE_G: 1.502107 Img_G: 0.201063 Per_G: 0.387576\n",
            " Batch [075/125] | Loss_D: 0.367095 Loss_G: 28.433603 | Valid_D: 0.361662 Fake_D: 0.372528 | BCE_G: 1.364844 Img_G: 0.194935 Per_G: 0.378764\n",
            " Batch [100/125] | Loss_D: 0.454212 Loss_G: 27.426723 | Valid_D: 0.397998 Fake_D: 0.510426 | BCE_G: 1.103945 Img_G: 0.186507 Per_G: 0.383602\n",
            " Batch [125/125] | Loss_D: 0.408709 Loss_G: 29.317043 | Valid_D: 0.359623 Fake_D: 0.457794 | BCE_G: 1.290527 Img_G: 0.207358 Per_G: 0.364537\n",
            " ----------------------------------------------------\n",
            " Epoch [026/100] | Loss_D: 0.493716 Loss_G: 28.918675 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [027/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.437749 Loss_G: 28.945656 | Valid_D: 0.386062 Fake_D: 0.489437 | BCE_G: 1.167031 Img_G: 0.202676 Per_G: 0.375551\n",
            " Batch [050/125] | Loss_D: 0.431797 Loss_G: 29.001732 | Valid_D: 0.520889 Fake_D: 0.342704 | BCE_G: 1.718953 Img_G: 0.197681 Per_G: 0.375735\n",
            " Batch [075/125] | Loss_D: 0.411785 Loss_G: 28.017590 | Valid_D: 0.406246 Fake_D: 0.417323 | BCE_G: 1.290853 Img_G: 0.193556 Per_G: 0.368556\n",
            " Batch [100/125] | Loss_D: 0.529758 Loss_G: 25.813055 | Valid_D: 0.666026 Fake_D: 0.393489 | BCE_G: 0.955138 Img_G: 0.182673 Per_G: 0.329530\n",
            " Batch [125/125] | Loss_D: 0.444731 Loss_G: 28.102915 | Valid_D: 0.473076 Fake_D: 0.416386 | BCE_G: 1.277736 Img_G: 0.194553 Per_G: 0.368493\n",
            " ----------------------------------------------------\n",
            " Epoch [027/100] | Loss_D: 0.495093 Loss_G: 28.662362 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [028/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.456920 Loss_G: 26.535719 | Valid_D: 0.404052 Fake_D: 0.509788 | BCE_G: 1.009873 Img_G: 0.183027 Per_G: 0.361156\n",
            " Batch [050/125] | Loss_D: 0.406035 Loss_G: 29.236557 | Valid_D: 0.342089 Fake_D: 0.469981 | BCE_G: 1.385365 Img_G: 0.206805 Per_G: 0.358536\n",
            " Batch [075/125] | Loss_D: 0.433830 Loss_G: 28.096510 | Valid_D: 0.493286 Fake_D: 0.374373 | BCE_G: 1.486487 Img_G: 0.193364 Per_G: 0.363682\n",
            " Batch [100/125] | Loss_D: 0.535950 Loss_G: 29.202969 | Valid_D: 0.464250 Fake_D: 0.607650 | BCE_G: 0.905764 Img_G: 0.205372 Per_G: 0.387998\n",
            " Batch [125/125] | Loss_D: 0.704748 Loss_G: 26.064714 | Valid_D: 1.038053 Fake_D: 0.371444 | BCE_G: 0.951709 Img_G: 0.185265 Per_G: 0.329326\n",
            " ----------------------------------------------------\n",
            " Epoch [028/100] | Loss_D: 0.496493 Loss_G: 28.341866 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [029/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.479633 Loss_G: 27.248882 | Valid_D: 0.599819 Fake_D: 0.359446 | BCE_G: 1.445224 Img_G: 0.182913 Per_G: 0.375618\n",
            " Batch [050/125] | Loss_D: 0.603134 Loss_G: 26.744219 | Valid_D: 0.442888 Fake_D: 0.763380 | BCE_G: 0.802486 Img_G: 0.182344 Per_G: 0.385368\n",
            " Batch [075/125] | Loss_D: 0.352612 Loss_G: 33.467720 | Valid_D: 0.343283 Fake_D: 0.361941 | BCE_G: 1.708907 Img_G: 0.243586 Per_G: 0.370010\n",
            " Batch [100/125] | Loss_D: 0.427955 Loss_G: 26.582558 | Valid_D: 0.367457 Fake_D: 0.488454 | BCE_G: 1.192584 Img_G: 0.180033 Per_G: 0.369333\n",
            " Batch [125/125] | Loss_D: 0.506907 Loss_G: 24.623566 | Valid_D: 0.579276 Fake_D: 0.434537 | BCE_G: 1.049508 Img_G: 0.165574 Per_G: 0.350830\n",
            " ----------------------------------------------------\n",
            " Epoch [029/100] | Loss_D: 0.500922 Loss_G: 27.859652 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [030/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.508133 Loss_G: 29.765068 | Valid_D: 0.586949 Fake_D: 0.429318 | BCE_G: 1.336410 Img_G: 0.208085 Per_G: 0.381006\n",
            " Batch [050/125] | Loss_D: 0.574558 Loss_G: 27.925072 | Valid_D: 0.373096 Fake_D: 0.776020 | BCE_G: 0.835125 Img_G: 0.194517 Per_G: 0.381911\n",
            " Batch [075/125] | Loss_D: 0.584880 Loss_G: 27.761652 | Valid_D: 0.380542 Fake_D: 0.789218 | BCE_G: 0.788276 Img_G: 0.195132 Per_G: 0.373008\n",
            " Batch [100/125] | Loss_D: 0.578956 Loss_G: 26.861879 | Valid_D: 0.576121 Fake_D: 0.581791 | BCE_G: 0.932281 Img_G: 0.188634 Per_G: 0.353311\n",
            " Batch [125/125] | Loss_D: 0.559029 Loss_G: 27.069998 | Valid_D: 0.593903 Fake_D: 0.524155 | BCE_G: 1.004808 Img_G: 0.187625 Per_G: 0.365132\n",
            " ----------------------------------------------------\n",
            " Epoch [030/100] | Loss_D: 0.509839 Loss_G: 27.638490 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [031/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.556643 Loss_G: 27.446476 | Valid_D: 0.362584 Fake_D: 0.750701 | BCE_G: 0.711215 Img_G: 0.192598 Per_G: 0.373772\n",
            " Batch [050/125] | Loss_D: 0.428657 Loss_G: 27.972595 | Valid_D: 0.468992 Fake_D: 0.388322 | BCE_G: 1.332996 Img_G: 0.191202 Per_G: 0.375969\n",
            " Batch [075/125] | Loss_D: 0.422269 Loss_G: 27.187336 | Valid_D: 0.456396 Fake_D: 0.388142 | BCE_G: 1.321381 Img_G: 0.186295 Per_G: 0.361821\n",
            " Batch [100/125] | Loss_D: 0.454518 Loss_G: 27.424461 | Valid_D: 0.367695 Fake_D: 0.541342 | BCE_G: 1.077467 Img_G: 0.186484 Per_G: 0.384929\n",
            " Batch [125/125] | Loss_D: 0.408123 Loss_G: 27.886930 | Valid_D: 0.357265 Fake_D: 0.458982 | BCE_G: 1.426846 Img_G: 0.184919 Per_G: 0.398407\n",
            " ----------------------------------------------------\n",
            " Epoch [031/100] | Loss_D: 0.513887 Loss_G: 27.399770 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [032/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.438274 Loss_G: 25.495857 | Valid_D: 0.398484 Fake_D: 0.478063 | BCE_G: 1.074155 Img_G: 0.174370 Per_G: 0.349233\n",
            " Batch [050/125] | Loss_D: 0.712015 Loss_G: 28.119970 | Valid_D: 0.433075 Fake_D: 0.990955 | BCE_G: 0.643098 Img_G: 0.206849 Per_G: 0.339598\n",
            " Batch [075/125] | Loss_D: 0.380141 Loss_G: 30.053459 | Valid_D: 0.335805 Fake_D: 0.424478 | BCE_G: 1.216629 Img_G: 0.211959 Per_G: 0.382045\n",
            " Batch [100/125] | Loss_D: 0.390086 Loss_G: 29.289494 | Valid_D: 0.339751 Fake_D: 0.440421 | BCE_G: 1.252183 Img_G: 0.206354 Per_G: 0.370097\n",
            " Batch [125/125] | Loss_D: 0.544842 Loss_G: 27.217836 | Valid_D: 0.654838 Fake_D: 0.434846 | BCE_G: 1.236003 Img_G: 0.189931 Per_G: 0.349435\n",
            " ----------------------------------------------------\n",
            " Epoch [032/100] | Loss_D: 0.520219 Loss_G: 27.266070 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [033/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.616668 Loss_G: 23.363586 | Valid_D: 0.577394 Fake_D: 0.655943 | BCE_G: 0.714231 Img_G: 0.160318 Per_G: 0.330880\n",
            " Batch [050/125] | Loss_D: 0.436838 Loss_G: 26.450455 | Valid_D: 0.452662 Fake_D: 0.421014 | BCE_G: 1.375692 Img_G: 0.174190 Per_G: 0.382786\n",
            " Batch [075/125] | Loss_D: 0.662776 Loss_G: 25.442650 | Valid_D: 0.713118 Fake_D: 0.612434 | BCE_G: 0.701681 Img_G: 0.183584 Per_G: 0.319126\n",
            " Batch [100/125] | Loss_D: 0.461673 Loss_G: 27.533350 | Valid_D: 0.337123 Fake_D: 0.586224 | BCE_G: 0.909224 Img_G: 0.192170 Per_G: 0.370356\n",
            " Batch [125/125] | Loss_D: 0.564524 Loss_G: 26.586567 | Valid_D: 0.698586 Fake_D: 0.430463 | BCE_G: 1.115077 Img_G: 0.191856 Per_G: 0.314292\n",
            " ----------------------------------------------------\n",
            " Epoch [033/100] | Loss_D: 0.526770 Loss_G: 26.991546 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [034/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.462159 Loss_G: 26.600363 | Valid_D: 0.397175 Fake_D: 0.527143 | BCE_G: 1.141917 Img_G: 0.182725 Per_G: 0.359296\n",
            " Batch [050/125] | Loss_D: 0.379299 Loss_G: 26.201851 | Valid_D: 0.375919 Fake_D: 0.382680 | BCE_G: 1.428673 Img_G: 0.174529 Per_G: 0.366013\n",
            " Batch [075/125] | Loss_D: 0.420000 Loss_G: 27.862387 | Valid_D: 0.346184 Fake_D: 0.493817 | BCE_G: 1.102694 Img_G: 0.196367 Per_G: 0.356149\n",
            " Batch [100/125] | Loss_D: 0.503117 Loss_G: 27.773617 | Valid_D: 0.432995 Fake_D: 0.573239 | BCE_G: 0.858530 Img_G: 0.195945 Per_G: 0.366029\n",
            " Batch [125/125] | Loss_D: 0.571614 Loss_G: 27.073486 | Valid_D: 0.726213 Fake_D: 0.417014 | BCE_G: 1.204692 Img_G: 0.183385 Per_G: 0.376515\n",
            " ----------------------------------------------------\n",
            " Epoch [034/100] | Loss_D: 0.528660 Loss_G: 26.789969 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [035/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.640870 Loss_G: 26.350868 | Valid_D: 0.896137 Fake_D: 0.385603 | BCE_G: 1.209414 Img_G: 0.177652 Per_G: 0.368810\n",
            " Batch [050/125] | Loss_D: 0.482159 Loss_G: 26.119209 | Valid_D: 0.367742 Fake_D: 0.596577 | BCE_G: 0.974823 Img_G: 0.177506 Per_G: 0.369691\n",
            " Batch [075/125] | Loss_D: 0.457762 Loss_G: 26.798981 | Valid_D: 0.371541 Fake_D: 0.543984 | BCE_G: 1.055358 Img_G: 0.184975 Per_G: 0.362306\n",
            " Batch [100/125] | Loss_D: 0.472679 Loss_G: 27.270409 | Valid_D: 0.343104 Fake_D: 0.602253 | BCE_G: 0.889837 Img_G: 0.191086 Per_G: 0.363600\n",
            " Batch [125/125] | Loss_D: 0.494006 Loss_G: 26.353743 | Valid_D: 0.646076 Fake_D: 0.341935 | BCE_G: 1.754994 Img_G: 0.173098 Per_G: 0.364446\n",
            " ----------------------------------------------------\n",
            " Epoch [035/100] | Loss_D: 0.532265 Loss_G: 26.404987 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [036/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.552376 Loss_G: 26.789629 | Valid_D: 0.730755 Fake_D: 0.373996 | BCE_G: 1.508731 Img_G: 0.182167 Per_G: 0.353211\n",
            " Batch [050/125] | Loss_D: 0.412279 Loss_G: 26.241488 | Valid_D: 0.362881 Fake_D: 0.461677 | BCE_G: 1.257722 Img_G: 0.176467 Per_G: 0.366852\n",
            " Batch [075/125] | Loss_D: 0.521436 Loss_G: 27.157192 | Valid_D: 0.579657 Fake_D: 0.463215 | BCE_G: 1.071276 Img_G: 0.189749 Per_G: 0.355553\n",
            " Batch [100/125] | Loss_D: 0.467484 Loss_G: 26.346628 | Valid_D: 0.336757 Fake_D: 0.598210 | BCE_G: 0.998439 Img_G: 0.183485 Per_G: 0.349985\n",
            " Batch [125/125] | Loss_D: 0.402633 Loss_G: 29.666771 | Valid_D: 0.337680 Fake_D: 0.467585 | BCE_G: 1.233263 Img_G: 0.208759 Per_G: 0.377882\n",
            " ----------------------------------------------------\n",
            " Epoch [036/100] | Loss_D: 0.538986 Loss_G: 26.258047 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [037/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.446768 Loss_G: 27.359455 | Valid_D: 0.344785 Fake_D: 0.548751 | BCE_G: 1.096636 Img_G: 0.185413 Per_G: 0.386078\n",
            " Batch [050/125] | Loss_D: 0.448234 Loss_G: 26.609873 | Valid_D: 0.429615 Fake_D: 0.466852 | BCE_G: 1.265160 Img_G: 0.176471 Per_G: 0.384881\n",
            " Batch [075/125] | Loss_D: 0.435664 Loss_G: 30.967737 | Valid_D: 0.367219 Fake_D: 0.504108 | BCE_G: 1.151767 Img_G: 0.224202 Per_G: 0.369787\n",
            " Batch [100/125] | Loss_D: 0.539853 Loss_G: 25.607635 | Valid_D: 0.350956 Fake_D: 0.728749 | BCE_G: 0.898093 Img_G: 0.171887 Per_G: 0.376042\n",
            " Batch [125/125] | Loss_D: 0.412531 Loss_G: 27.179838 | Valid_D: 0.450118 Fake_D: 0.374944 | BCE_G: 1.552592 Img_G: 0.183240 Per_G: 0.365161\n",
            " ----------------------------------------------------\n",
            " Epoch [037/100] | Loss_D: 0.538683 Loss_G: 26.068849 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [038/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.518013 Loss_G: 26.825325 | Valid_D: 0.350030 Fake_D: 0.685995 | BCE_G: 0.873708 Img_G: 0.189379 Per_G: 0.350688\n",
            " Batch [050/125] | Loss_D: 0.641376 Loss_G: 25.513260 | Valid_D: 0.816761 Fake_D: 0.465992 | BCE_G: 1.054309 Img_G: 0.173963 Per_G: 0.353131\n",
            " Batch [075/125] | Loss_D: 0.554707 Loss_G: 25.385799 | Valid_D: 0.593220 Fake_D: 0.516193 | BCE_G: 0.967821 Img_G: 0.170450 Per_G: 0.368650\n",
            " Batch [100/125] | Loss_D: 0.506204 Loss_G: 24.707508 | Valid_D: 0.449563 Fake_D: 0.562844 | BCE_G: 1.013116 Img_G: 0.165571 Per_G: 0.356863\n",
            " Batch [125/125] | Loss_D: 0.349755 Loss_G: 27.191605 | Valid_D: 0.354338 Fake_D: 0.345173 | BCE_G: 1.749047 Img_G: 0.181625 Per_G: 0.364003\n",
            " ----------------------------------------------------\n",
            " Epoch [038/100] | Loss_D: 0.545924 Loss_G: 26.015915 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [039/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.486548 Loss_G: 25.720016 | Valid_D: 0.525668 Fake_D: 0.447429 | BCE_G: 1.195769 Img_G: 0.171246 Per_G: 0.369984\n",
            " Batch [050/125] | Loss_D: 0.460524 Loss_G: 26.872643 | Valid_D: 0.502718 Fake_D: 0.418331 | BCE_G: 1.416447 Img_G: 0.183348 Per_G: 0.356070\n",
            " Batch [075/125] | Loss_D: 0.577058 Loss_G: 25.742466 | Valid_D: 0.333557 Fake_D: 0.820560 | BCE_G: 0.780683 Img_G: 0.179993 Per_G: 0.348123\n",
            " Batch [100/125] | Loss_D: 0.578025 Loss_G: 27.033123 | Valid_D: 0.752936 Fake_D: 0.403115 | BCE_G: 1.291299 Img_G: 0.183102 Per_G: 0.371580\n",
            " Batch [125/125] | Loss_D: 0.428786 Loss_G: 26.813934 | Valid_D: 0.343326 Fake_D: 0.514246 | BCE_G: 1.198821 Img_G: 0.185488 Per_G: 0.353315\n",
            " ----------------------------------------------------\n",
            " Epoch [039/100] | Loss_D: 0.541044 Loss_G: 25.623429 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [040/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.516094 Loss_G: 24.729094 | Valid_D: 0.513677 Fake_D: 0.518511 | BCE_G: 0.958578 Img_G: 0.166451 Per_G: 0.356273\n",
            " Batch [050/125] | Loss_D: 0.550428 Loss_G: 24.276957 | Valid_D: 0.620395 Fake_D: 0.480461 | BCE_G: 1.021625 Img_G: 0.159612 Per_G: 0.364708\n",
            " Batch [075/125] | Loss_D: 0.483097 Loss_G: 25.818581 | Valid_D: 0.439842 Fake_D: 0.526352 | BCE_G: 0.884982 Img_G: 0.179884 Per_G: 0.347258\n",
            " Batch [100/125] | Loss_D: 0.423986 Loss_G: 25.870348 | Valid_D: 0.351353 Fake_D: 0.496618 | BCE_G: 1.122772 Img_G: 0.172091 Per_G: 0.376923\n",
            " Batch [125/125] | Loss_D: 0.460895 Loss_G: 24.970837 | Valid_D: 0.365107 Fake_D: 0.556684 | BCE_G: 0.956996 Img_G: 0.167490 Per_G: 0.363240\n",
            " ----------------------------------------------------\n",
            " Epoch [040/100] | Loss_D: 0.549295 Loss_G: 25.500710 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [041/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.666538 Loss_G: 25.893124 | Valid_D: 0.984983 Fake_D: 0.348092 | BCE_G: 1.676821 Img_G: 0.173431 Per_G: 0.343660\n",
            " Batch [050/125] | Loss_D: 0.722576 Loss_G: 24.596737 | Valid_D: 1.038776 Fake_D: 0.406376 | BCE_G: 1.113007 Img_G: 0.158804 Per_G: 0.380164\n",
            " Batch [075/125] | Loss_D: 0.626788 Loss_G: 23.869335 | Valid_D: 0.567440 Fake_D: 0.686136 | BCE_G: 0.694288 Img_G: 0.154830 Per_G: 0.384603\n",
            " Batch [100/125] | Loss_D: 0.512278 Loss_G: 24.787292 | Valid_D: 0.482904 Fake_D: 0.541653 | BCE_G: 0.966635 Img_G: 0.168247 Per_G: 0.349800\n",
            " Batch [125/125] | Loss_D: 0.698063 Loss_G: 24.368862 | Valid_D: 0.782341 Fake_D: 0.613786 | BCE_G: 0.823217 Img_G: 0.167682 Per_G: 0.338874\n",
            " ----------------------------------------------------\n",
            " Epoch [041/100] | Loss_D: 0.549876 Loss_G: 25.357812 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [042/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.341811 Loss_G: 26.463511 | Valid_D: 0.336061 Fake_D: 0.347562 | BCE_G: 1.645283 Img_G: 0.177032 Per_G: 0.355754\n",
            " Batch [050/125] | Loss_D: 0.419863 Loss_G: 26.390476 | Valid_D: 0.435753 Fake_D: 0.403973 | BCE_G: 1.333927 Img_G: 0.178928 Per_G: 0.358186\n",
            " Batch [075/125] | Loss_D: 0.496321 Loss_G: 24.582592 | Valid_D: 0.621610 Fake_D: 0.371032 | BCE_G: 1.334803 Img_G: 0.161408 Per_G: 0.355347\n",
            " Batch [100/125] | Loss_D: 0.628802 Loss_G: 24.737698 | Valid_D: 0.897892 Fake_D: 0.359712 | BCE_G: 1.257538 Img_G: 0.167472 Per_G: 0.336648\n",
            " Batch [125/125] | Loss_D: 0.522765 Loss_G: 24.707706 | Valid_D: 0.449052 Fake_D: 0.596479 | BCE_G: 0.853201 Img_G: 0.165232 Per_G: 0.366566\n",
            " ----------------------------------------------------\n",
            " Epoch [042/100] | Loss_D: 0.549707 Loss_G: 25.296447 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [043/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.576323 Loss_G: 25.073446 | Valid_D: 0.350908 Fake_D: 0.801738 | BCE_G: 0.790305 Img_G: 0.171782 Per_G: 0.355245\n",
            " Batch [050/125] | Loss_D: 0.645920 Loss_G: 25.081573 | Valid_D: 0.385076 Fake_D: 0.906764 | BCE_G: 0.667188 Img_G: 0.174510 Per_G: 0.348171\n",
            " Batch [075/125] | Loss_D: 0.719010 Loss_G: 23.177902 | Valid_D: 0.927842 Fake_D: 0.510178 | BCE_G: 0.832714 Img_G: 0.155779 Per_G: 0.338365\n",
            " Batch [100/125] | Loss_D: 0.387053 Loss_G: 26.403679 | Valid_D: 0.389140 Fake_D: 0.384967 | BCE_G: 1.425490 Img_G: 0.172371 Per_G: 0.387053\n",
            " Batch [125/125] | Loss_D: 0.427904 Loss_G: 27.165150 | Valid_D: 0.443776 Fake_D: 0.412032 | BCE_G: 1.414366 Img_G: 0.186426 Per_G: 0.355412\n",
            " ----------------------------------------------------\n",
            " Epoch [043/100] | Loss_D: 0.549216 Loss_G: 24.953972 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [044/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.649644 Loss_G: 23.686888 | Valid_D: 0.536971 Fake_D: 0.762317 | BCE_G: 0.632169 Img_G: 0.158161 Per_G: 0.361932\n",
            " Batch [050/125] | Loss_D: 0.442335 Loss_G: 25.279345 | Valid_D: 0.399472 Fake_D: 0.485198 | BCE_G: 1.098608 Img_G: 0.172476 Per_G: 0.346657\n",
            " Batch [075/125] | Loss_D: 0.464131 Loss_G: 24.200930 | Valid_D: 0.410522 Fake_D: 0.517740 | BCE_G: 1.063481 Img_G: 0.161079 Per_G: 0.351476\n",
            " Batch [100/125] | Loss_D: 0.567280 Loss_G: 24.981983 | Valid_D: 0.789516 Fake_D: 0.345044 | BCE_G: 1.506874 Img_G: 0.163943 Per_G: 0.354040\n",
            " Batch [125/125] | Loss_D: 0.802578 Loss_G: 23.410320 | Valid_D: 1.251211 Fake_D: 0.353946 | BCE_G: 1.457065 Img_G: 0.146302 Per_G: 0.366152\n",
            " ----------------------------------------------------\n",
            " Epoch [044/100] | Loss_D: 0.560722 Loss_G: 24.887093 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [045/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.574647 Loss_G: 23.714195 | Valid_D: 0.683118 Fake_D: 0.466175 | BCE_G: 1.042837 Img_G: 0.160740 Per_G: 0.329865\n",
            " Batch [050/125] | Loss_D: 0.503024 Loss_G: 23.741425 | Valid_D: 0.554612 Fake_D: 0.451436 | BCE_G: 1.078128 Img_G: 0.154115 Per_G: 0.362591\n",
            " Batch [075/125] | Loss_D: 0.679168 Loss_G: 24.784710 | Valid_D: 0.417943 Fake_D: 0.940393 | BCE_G: 0.624636 Img_G: 0.168609 Per_G: 0.364961\n",
            " Batch [100/125] | Loss_D: 0.396509 Loss_G: 25.477875 | Valid_D: 0.382512 Fake_D: 0.410507 | BCE_G: 1.254628 Img_G: 0.168841 Per_G: 0.366957\n",
            " Batch [125/125] | Loss_D: 0.446673 Loss_G: 25.312576 | Valid_D: 0.525258 Fake_D: 0.368088 | BCE_G: 1.532552 Img_G: 0.166229 Per_G: 0.357858\n",
            " ----------------------------------------------------\n",
            " Epoch [045/100] | Loss_D: 0.556678 Loss_G: 24.659689 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [046/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.539033 Loss_G: 23.564320 | Valid_D: 0.357566 Fake_D: 0.720500 | BCE_G: 0.808456 Img_G: 0.153869 Per_G: 0.368449\n",
            " Batch [050/125] | Loss_D: 0.693630 Loss_G: 22.821011 | Valid_D: 0.875749 Fake_D: 0.511512 | BCE_G: 0.914835 Img_G: 0.150160 Per_G: 0.344510\n",
            " Batch [075/125] | Loss_D: 0.645727 Loss_G: 25.401772 | Valid_D: 0.755404 Fake_D: 0.536051 | BCE_G: 0.841075 Img_G: 0.174430 Per_G: 0.355886\n",
            " Batch [100/125] | Loss_D: 0.513206 Loss_G: 24.564184 | Valid_D: 0.466747 Fake_D: 0.559665 | BCE_G: 0.903153 Img_G: 0.160473 Per_G: 0.380687\n",
            " Batch [125/125] | Loss_D: 0.535990 Loss_G: 24.340750 | Valid_D: 0.579392 Fake_D: 0.492588 | BCE_G: 1.074670 Img_G: 0.162161 Per_G: 0.352497\n",
            " ----------------------------------------------------\n",
            " Epoch [046/100] | Loss_D: 0.545467 Loss_G: 24.454983 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [047/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.658985 Loss_G: 23.821770 | Valid_D: 0.822680 Fake_D: 0.495289 | BCE_G: 0.887823 Img_G: 0.159158 Per_G: 0.350907\n",
            " Batch [050/125] | Loss_D: 0.501414 Loss_G: 24.681656 | Valid_D: 0.527717 Fake_D: 0.475112 | BCE_G: 1.140452 Img_G: 0.159283 Per_G: 0.380643\n",
            " Batch [075/125] | Loss_D: 0.413467 Loss_G: 25.659687 | Valid_D: 0.464688 Fake_D: 0.362246 | BCE_G: 1.600497 Img_G: 0.172369 Per_G: 0.341113\n",
            " Batch [100/125] | Loss_D: 0.359138 Loss_G: 26.672348 | Valid_D: 0.363566 Fake_D: 0.354710 | BCE_G: 1.682629 Img_G: 0.175639 Per_G: 0.371290\n",
            " Batch [125/125] | Loss_D: 0.614848 Loss_G: 23.972731 | Valid_D: 0.891735 Fake_D: 0.337961 | BCE_G: 1.640595 Img_G: 0.156928 Per_G: 0.331966\n",
            " ----------------------------------------------------\n",
            " Epoch [047/100] | Loss_D: 0.558838 Loss_G: 24.563496 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [048/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.584021 Loss_G: 24.228271 | Valid_D: 0.455904 Fake_D: 0.712138 | BCE_G: 0.884185 Img_G: 0.165268 Per_G: 0.340864\n",
            " Batch [050/125] | Loss_D: 0.435214 Loss_G: 25.391169 | Valid_D: 0.517090 Fake_D: 0.353338 | BCE_G: 1.603340 Img_G: 0.167152 Per_G: 0.353632\n",
            " Batch [075/125] | Loss_D: 0.423806 Loss_G: 24.595425 | Valid_D: 0.389465 Fake_D: 0.458147 | BCE_G: 1.240542 Img_G: 0.166132 Per_G: 0.337083\n",
            " Batch [100/125] | Loss_D: 0.435791 Loss_G: 24.404953 | Valid_D: 0.479024 Fake_D: 0.392559 | BCE_G: 1.267250 Img_G: 0.160278 Per_G: 0.355495\n",
            " Batch [125/125] | Loss_D: 0.439771 Loss_G: 25.770878 | Valid_D: 0.335181 Fake_D: 0.544361 | BCE_G: 1.117731 Img_G: 0.170464 Per_G: 0.380338\n",
            " ----------------------------------------------------\n",
            " Epoch [048/100] | Loss_D: 0.555857 Loss_G: 24.326605 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [049/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.626259 Loss_G: 23.932041 | Valid_D: 0.873358 Fake_D: 0.379159 | BCE_G: 1.245522 Img_G: 0.157846 Per_G: 0.345097\n",
            " Batch [050/125] | Loss_D: 0.638351 Loss_G: 24.133579 | Valid_D: 0.880843 Fake_D: 0.395858 | BCE_G: 1.197842 Img_G: 0.157371 Per_G: 0.359933\n",
            " Batch [075/125] | Loss_D: 0.526329 Loss_G: 23.980457 | Valid_D: 0.350601 Fake_D: 0.702058 | BCE_G: 0.875521 Img_G: 0.165093 Per_G: 0.329781\n",
            " Batch [100/125] | Loss_D: 0.441177 Loss_G: 24.422531 | Valid_D: 0.352346 Fake_D: 0.530008 | BCE_G: 1.074041 Img_G: 0.162534 Per_G: 0.354756\n",
            " Batch [125/125] | Loss_D: 0.625329 Loss_G: 23.641741 | Valid_D: 0.697950 Fake_D: 0.552708 | BCE_G: 0.892504 Img_G: 0.151758 Per_G: 0.378671\n",
            " ----------------------------------------------------\n",
            " Epoch [049/100] | Loss_D: 0.559026 Loss_G: 24.188106 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [050/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.793539 Loss_G: 24.141308 | Valid_D: 0.338017 Fake_D: 1.249062 | BCE_G: 0.526249 Img_G: 0.163110 Per_G: 0.365202\n",
            " Batch [050/125] | Loss_D: 0.518700 Loss_G: 23.936459 | Valid_D: 0.347283 Fake_D: 0.690116 | BCE_G: 0.871468 Img_G: 0.158052 Per_G: 0.362989\n",
            " Batch [075/125] | Loss_D: 0.505769 Loss_G: 22.778915 | Valid_D: 0.514599 Fake_D: 0.496939 | BCE_G: 0.969187 Img_G: 0.146001 Per_G: 0.360483\n",
            " Batch [100/125] | Loss_D: 0.547192 Loss_G: 24.452835 | Valid_D: 0.598637 Fake_D: 0.495746 | BCE_G: 1.097478 Img_G: 0.162476 Per_G: 0.355390\n",
            " Batch [125/125] | Loss_D: 0.418660 Loss_G: 24.513826 | Valid_D: 0.443071 Fake_D: 0.394249 | BCE_G: 1.270834 Img_G: 0.159540 Per_G: 0.364447\n",
            " ----------------------------------------------------\n",
            " Epoch [050/100] | Loss_D: 0.552476 Loss_G: 23.925489 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [051/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.502015 Loss_G: 24.131958 | Valid_D: 0.504657 Fake_D: 0.499372 | BCE_G: 1.056103 Img_G: 0.162205 Per_G: 0.342769\n",
            " Batch [050/125] | Loss_D: 0.504159 Loss_G: 23.214880 | Valid_D: 0.457230 Fake_D: 0.551089 | BCE_G: 0.939455 Img_G: 0.155373 Per_G: 0.336904\n",
            " Batch [075/125] | Loss_D: 0.435214 Loss_G: 23.821619 | Valid_D: 0.494837 Fake_D: 0.375592 | BCE_G: 1.447649 Img_G: 0.154514 Per_G: 0.346129\n",
            " Batch [100/125] | Loss_D: 0.429142 Loss_G: 23.837564 | Valid_D: 0.365242 Fake_D: 0.493043 | BCE_G: 1.101536 Img_G: 0.157800 Per_G: 0.347801\n",
            " Batch [125/125] | Loss_D: 0.811653 Loss_G: 23.818722 | Valid_D: 0.475177 Fake_D: 1.148130 | BCE_G: 0.588476 Img_G: 0.164920 Per_G: 0.336913\n",
            " ----------------------------------------------------\n",
            " Epoch [051/100] | Loss_D: 0.552930 Loss_G: 23.835022 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [052/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.770631 Loss_G: 22.364735 | Valid_D: 1.112570 Fake_D: 0.428693 | BCE_G: 1.123647 Img_G: 0.144782 Per_G: 0.338143\n",
            " Batch [050/125] | Loss_D: 0.465421 Loss_G: 23.928492 | Valid_D: 0.467210 Fake_D: 0.463633 | BCE_G: 1.141996 Img_G: 0.154261 Per_G: 0.368020\n",
            " Batch [075/125] | Loss_D: 0.797236 Loss_G: 23.702515 | Valid_D: 1.013221 Fake_D: 0.581251 | BCE_G: 0.905460 Img_G: 0.153882 Per_G: 0.370444\n",
            " Batch [100/125] | Loss_D: 0.718256 Loss_G: 22.328251 | Valid_D: 1.017852 Fake_D: 0.418660 | BCE_G: 1.194493 Img_G: 0.143971 Per_G: 0.336830\n",
            " Batch [125/125] | Loss_D: 0.506434 Loss_G: 22.138287 | Valid_D: 0.633152 Fake_D: 0.379716 | BCE_G: 1.186140 Img_G: 0.147914 Per_G: 0.308037\n",
            " ----------------------------------------------------\n",
            " Epoch [052/100] | Loss_D: 0.566346 Loss_G: 23.689882 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [053/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.571473 Loss_G: 22.325985 | Valid_D: 0.575537 Fake_D: 0.567409 | BCE_G: 0.955934 Img_G: 0.146818 Per_G: 0.334414\n",
            " Batch [050/125] | Loss_D: 0.536395 Loss_G: 23.201778 | Valid_D: 0.678365 Fake_D: 0.394426 | BCE_G: 1.292058 Img_G: 0.149336 Per_G: 0.348805\n",
            " Batch [075/125] | Loss_D: 0.414955 Loss_G: 24.330902 | Valid_D: 0.402909 Fake_D: 0.427000 | BCE_G: 1.270833 Img_G: 0.161269 Per_G: 0.346657\n",
            " Batch [100/125] | Loss_D: 0.520593 Loss_G: 22.894360 | Valid_D: 0.584637 Fake_D: 0.456549 | BCE_G: 1.073371 Img_G: 0.151282 Per_G: 0.334637\n",
            " Batch [125/125] | Loss_D: 0.715729 Loss_G: 22.644312 | Valid_D: 0.398433 Fake_D: 1.033024 | BCE_G: 0.635385 Img_G: 0.148726 Per_G: 0.356817\n",
            " ----------------------------------------------------\n",
            " Epoch [053/100] | Loss_D: 0.550002 Loss_G: 23.610176 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [054/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.386459 Loss_G: 23.665989 | Valid_D: 0.358628 Fake_D: 0.414291 | BCE_G: 1.142687 Img_G: 0.162205 Per_G: 0.315141\n",
            " Batch [050/125] | Loss_D: 0.496690 Loss_G: 23.719677 | Valid_D: 0.338333 Fake_D: 0.655048 | BCE_G: 0.892420 Img_G: 0.153910 Per_G: 0.371814\n",
            " Batch [075/125] | Loss_D: 0.501468 Loss_G: 24.140528 | Valid_D: 0.452240 Fake_D: 0.550697 | BCE_G: 0.947191 Img_G: 0.163351 Per_G: 0.342910\n",
            " Batch [100/125] | Loss_D: 0.496538 Loss_G: 24.668636 | Valid_D: 0.398717 Fake_D: 0.594358 | BCE_G: 0.943764 Img_G: 0.164030 Per_G: 0.366094\n",
            " Batch [125/125] | Loss_D: 0.677353 Loss_G: 22.816742 | Valid_D: 0.535937 Fake_D: 0.818768 | BCE_G: 0.721528 Img_G: 0.148048 Per_G: 0.364518\n",
            " ----------------------------------------------------\n",
            " Epoch [054/100] | Loss_D: 0.556467 Loss_G: 23.395541 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [055/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.572308 Loss_G: 24.282284 | Valid_D: 0.397895 Fake_D: 0.746721 | BCE_G: 0.763265 Img_G: 0.166390 Per_G: 0.344002\n",
            " Batch [050/125] | Loss_D: 0.544494 Loss_G: 22.247215 | Valid_D: 0.470496 Fake_D: 0.618492 | BCE_G: 0.857502 Img_G: 0.144151 Per_G: 0.348729\n",
            " Batch [075/125] | Loss_D: 0.629401 Loss_G: 23.350857 | Valid_D: 0.338104 Fake_D: 0.920697 | BCE_G: 0.661631 Img_G: 0.155198 Per_G: 0.358472\n",
            " Batch [100/125] | Loss_D: 0.479905 Loss_G: 23.095474 | Valid_D: 0.427528 Fake_D: 0.532283 | BCE_G: 1.100311 Img_G: 0.149437 Per_G: 0.352571\n",
            " Batch [125/125] | Loss_D: 0.705727 Loss_G: 21.482918 | Valid_D: 0.511420 Fake_D: 0.900033 | BCE_G: 0.707076 Img_G: 0.139438 Per_G: 0.341600\n",
            " ----------------------------------------------------\n",
            " Epoch [055/100] | Loss_D: 0.562126 Loss_G: 23.306962 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [056/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.417470 Loss_G: 24.103580 | Valid_D: 0.431537 Fake_D: 0.403402 | BCE_G: 1.269653 Img_G: 0.158077 Per_G: 0.351312\n",
            " Batch [050/125] | Loss_D: 0.443836 Loss_G: 24.550941 | Valid_D: 0.353903 Fake_D: 0.533769 | BCE_G: 1.074425 Img_G: 0.161920 Per_G: 0.364226\n",
            " Batch [075/125] | Loss_D: 0.492054 Loss_G: 23.189043 | Valid_D: 0.475996 Fake_D: 0.508113 | BCE_G: 1.007711 Img_G: 0.153674 Per_G: 0.340697\n",
            " Batch [100/125] | Loss_D: 0.517108 Loss_G: 23.711769 | Valid_D: 0.695588 Fake_D: 0.338629 | BCE_G: 1.609101 Img_G: 0.147074 Per_G: 0.369765\n",
            " Batch [125/125] | Loss_D: 0.409175 Loss_G: 24.075294 | Valid_D: 0.377762 Fake_D: 0.440587 | BCE_G: 1.312073 Img_G: 0.157976 Per_G: 0.348282\n",
            " ----------------------------------------------------\n",
            " Epoch [056/100] | Loss_D: 0.559156 Loss_G: 23.194204 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [057/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.446113 Loss_G: 23.270765 | Valid_D: 0.405961 Fake_D: 0.486264 | BCE_G: 1.044371 Img_G: 0.156224 Per_G: 0.330202\n",
            " Batch [050/125] | Loss_D: 0.888738 Loss_G: 22.308123 | Valid_D: 1.424345 Fake_D: 0.353130 | BCE_G: 1.359293 Img_G: 0.142758 Per_G: 0.333653\n",
            " Batch [075/125] | Loss_D: 0.541711 Loss_G: 24.516602 | Valid_D: 0.521248 Fake_D: 0.562174 | BCE_G: 0.863465 Img_G: 0.163055 Per_G: 0.367380\n",
            " Batch [100/125] | Loss_D: 0.480478 Loss_G: 24.342834 | Valid_D: 0.466412 Fake_D: 0.494543 | BCE_G: 0.998810 Img_G: 0.159323 Per_G: 0.370589\n",
            " Batch [125/125] | Loss_D: 0.365907 Loss_G: 26.960642 | Valid_D: 0.334847 Fake_D: 0.396968 | BCE_G: 1.448504 Img_G: 0.184750 Per_G: 0.351855\n",
            " ----------------------------------------------------\n",
            " Epoch [057/100] | Loss_D: 0.559087 Loss_G: 23.167358 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [058/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.657988 Loss_G: 21.273411 | Valid_D: 0.788421 Fake_D: 0.527555 | BCE_G: 0.825436 Img_G: 0.139884 Per_G: 0.322977\n",
            " Batch [050/125] | Loss_D: 0.446519 Loss_G: 25.648144 | Valid_D: 0.336474 Fake_D: 0.556564 | BCE_G: 1.097070 Img_G: 0.172239 Per_G: 0.366359\n",
            " Batch [075/125] | Loss_D: 0.497711 Loss_G: 23.709476 | Valid_D: 0.557916 Fake_D: 0.437506 | BCE_G: 1.188111 Img_G: 0.160324 Per_G: 0.324448\n",
            " Batch [100/125] | Loss_D: 0.794626 Loss_G: 21.515614 | Valid_D: 1.245593 Fake_D: 0.343658 | BCE_G: 1.413566 Img_G: 0.137618 Per_G: 0.317015\n",
            " Batch [125/125] | Loss_D: 0.472800 Loss_G: 23.122055 | Valid_D: 0.527622 Fake_D: 0.417978 | BCE_G: 1.110689 Img_G: 0.152712 Per_G: 0.337007\n",
            " ----------------------------------------------------\n",
            " Epoch [058/100] | Loss_D: 0.565154 Loss_G: 23.072263 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [059/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.585641 Loss_G: 23.771694 | Valid_D: 0.523530 Fake_D: 0.647752 | BCE_G: 0.765355 Img_G: 0.163436 Per_G: 0.333136\n",
            " Batch [050/125] | Loss_D: 0.505257 Loss_G: 23.242489 | Valid_D: 0.544556 Fake_D: 0.465957 | BCE_G: 1.148466 Img_G: 0.149683 Per_G: 0.356287\n",
            " Batch [075/125] | Loss_D: 0.523970 Loss_G: 24.304657 | Valid_D: 0.343281 Fake_D: 0.704659 | BCE_G: 0.804278 Img_G: 0.167644 Per_G: 0.336801\n",
            " Batch [100/125] | Loss_D: 0.616781 Loss_G: 21.241812 | Valid_D: 0.666016 Fake_D: 0.567546 | BCE_G: 0.900522 Img_G: 0.137959 Per_G: 0.327268\n",
            " Batch [125/125] | Loss_D: 0.558114 Loss_G: 21.263472 | Valid_D: 0.499365 Fake_D: 0.616863 | BCE_G: 0.911960 Img_G: 0.134210 Per_G: 0.346525\n",
            " ----------------------------------------------------\n",
            " Epoch [059/100] | Loss_D: 0.556658 Loss_G: 22.787367 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [060/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.736862 Loss_G: 22.110039 | Valid_D: 0.405512 Fake_D: 1.068211 | BCE_G: 0.594586 Img_G: 0.147363 Per_G: 0.338956\n",
            " Batch [050/125] | Loss_D: 0.580221 Loss_G: 22.619356 | Valid_D: 0.690672 Fake_D: 0.469770 | BCE_G: 1.184002 Img_G: 0.144898 Per_G: 0.347280\n",
            " Batch [075/125] | Loss_D: 0.533947 Loss_G: 22.739124 | Valid_D: 0.434181 Fake_D: 0.633714 | BCE_G: 0.845138 Img_G: 0.149560 Per_G: 0.346899\n",
            " Batch [100/125] | Loss_D: 0.489130 Loss_G: 23.337128 | Valid_D: 0.588925 Fake_D: 0.389335 | BCE_G: 1.343346 Img_G: 0.151328 Per_G: 0.343047\n",
            " Batch [125/125] | Loss_D: 0.877940 Loss_G: 21.472712 | Valid_D: 1.314589 Fake_D: 0.441290 | BCE_G: 1.101207 Img_G: 0.137042 Per_G: 0.333367\n",
            " ----------------------------------------------------\n",
            " Epoch [060/100] | Loss_D: 0.561787 Loss_G: 22.711532 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [061/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.502901 Loss_G: 23.899700 | Valid_D: 0.608442 Fake_D: 0.397360 | BCE_G: 1.344577 Img_G: 0.156663 Per_G: 0.344440\n",
            " Batch [050/125] | Loss_D: 0.427537 Loss_G: 22.399225 | Valid_D: 0.430537 Fake_D: 0.424538 | BCE_G: 1.176356 Img_G: 0.146369 Per_G: 0.329298\n",
            " Batch [075/125] | Loss_D: 0.561078 Loss_G: 22.485764 | Valid_D: 0.735745 Fake_D: 0.386411 | BCE_G: 1.321531 Img_G: 0.145455 Per_G: 0.330938\n",
            " Batch [100/125] | Loss_D: 0.460134 Loss_G: 24.048679 | Valid_D: 0.572825 Fake_D: 0.347444 | BCE_G: 1.599207 Img_G: 0.154729 Per_G: 0.348829\n",
            " Batch [125/125] | Loss_D: 0.462575 Loss_G: 23.187111 | Valid_D: 0.371603 Fake_D: 0.553548 | BCE_G: 0.991697 Img_G: 0.151521 Per_G: 0.352165\n",
            " ----------------------------------------------------\n",
            " Epoch [061/100] | Loss_D: 0.552552 Loss_G: 22.575463 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [062/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.524501 Loss_G: 21.880909 | Valid_D: 0.396236 Fake_D: 0.652765 | BCE_G: 0.779504 Img_G: 0.147646 Per_G: 0.316838\n",
            " Batch [050/125] | Loss_D: 0.477029 Loss_G: 22.424728 | Valid_D: 0.430167 Fake_D: 0.523891 | BCE_G: 1.047812 Img_G: 0.151845 Per_G: 0.309619\n",
            " Batch [075/125] | Loss_D: 0.471326 Loss_G: 23.574581 | Valid_D: 0.460777 Fake_D: 0.481874 | BCE_G: 1.057226 Img_G: 0.151606 Per_G: 0.367839\n",
            " Batch [100/125] | Loss_D: 0.738080 Loss_G: 22.365295 | Valid_D: 0.983356 Fake_D: 0.492805 | BCE_G: 0.829775 Img_G: 0.146855 Per_G: 0.342501\n",
            " Batch [125/125] | Loss_D: 0.465025 Loss_G: 23.377476 | Valid_D: 0.339511 Fake_D: 0.590538 | BCE_G: 1.101040 Img_G: 0.155090 Per_G: 0.338373\n",
            " ----------------------------------------------------\n",
            " Epoch [062/100] | Loss_D: 0.555462 Loss_G: 22.521361 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [063/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.425167 Loss_G: 23.548264 | Valid_D: 0.334630 Fake_D: 0.515703 | BCE_G: 0.990475 Img_G: 0.161603 Per_G: 0.319873\n",
            " Batch [050/125] | Loss_D: 0.514356 Loss_G: 22.958668 | Valid_D: 0.554126 Fake_D: 0.474585 | BCE_G: 1.129954 Img_G: 0.148127 Per_G: 0.350803\n",
            " Batch [075/125] | Loss_D: 0.437086 Loss_G: 22.068481 | Valid_D: 0.395380 Fake_D: 0.478791 | BCE_G: 1.172361 Img_G: 0.140744 Per_G: 0.341087\n",
            " Batch [100/125] | Loss_D: 0.584128 Loss_G: 23.544590 | Valid_D: 0.341528 Fake_D: 0.826728 | BCE_G: 0.793505 Img_G: 0.156381 Per_G: 0.355650\n",
            " Batch [125/125] | Loss_D: 0.535929 Loss_G: 22.323874 | Valid_D: 0.694188 Fake_D: 0.377671 | BCE_G: 1.318887 Img_G: 0.140613 Per_G: 0.347184\n",
            " ----------------------------------------------------\n",
            " Epoch [063/100] | Loss_D: 0.557073 Loss_G: 22.362541 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [064/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.503362 Loss_G: 22.651815 | Valid_D: 0.446927 Fake_D: 0.559798 | BCE_G: 0.978805 Img_G: 0.150555 Per_G: 0.330878\n",
            " Batch [050/125] | Loss_D: 0.604521 Loss_G: 22.055386 | Valid_D: 0.643807 Fake_D: 0.565236 | BCE_G: 0.862787 Img_G: 0.147602 Per_G: 0.321618\n",
            " Batch [075/125] | Loss_D: 0.470363 Loss_G: 21.628649 | Valid_D: 0.585333 Fake_D: 0.355393 | BCE_G: 1.321419 Img_G: 0.140065 Per_G: 0.315039\n",
            " Batch [100/125] | Loss_D: 0.789357 Loss_G: 22.231125 | Valid_D: 0.927444 Fake_D: 0.651269 | BCE_G: 0.831146 Img_G: 0.149673 Per_G: 0.321633\n",
            " Batch [125/125] | Loss_D: 0.667160 Loss_G: 21.388464 | Valid_D: 0.835610 Fake_D: 0.498709 | BCE_G: 0.909714 Img_G: 0.138548 Per_G: 0.331197\n",
            " ----------------------------------------------------\n",
            " Epoch [064/100] | Loss_D: 0.559929 Loss_G: 22.279609 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [065/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.432212 Loss_G: 23.210812 | Valid_D: 0.384018 Fake_D: 0.480406 | BCE_G: 1.084829 Img_G: 0.155233 Per_G: 0.330132\n",
            " Batch [050/125] | Loss_D: 0.417221 Loss_G: 23.777414 | Valid_D: 0.340854 Fake_D: 0.493588 | BCE_G: 1.106627 Img_G: 0.155928 Per_G: 0.353901\n",
            " Batch [075/125] | Loss_D: 0.547091 Loss_G: 22.274113 | Valid_D: 0.427205 Fake_D: 0.666977 | BCE_G: 0.779969 Img_G: 0.150032 Per_G: 0.324549\n",
            " Batch [100/125] | Loss_D: 0.674393 Loss_G: 21.656033 | Valid_D: 0.504352 Fake_D: 0.844434 | BCE_G: 0.628578 Img_G: 0.144778 Per_G: 0.327484\n",
            " Batch [125/125] | Loss_D: 0.647041 Loss_G: 21.311291 | Valid_D: 0.897933 Fake_D: 0.396149 | BCE_G: 1.246443 Img_G: 0.137242 Per_G: 0.317033\n",
            " ----------------------------------------------------\n",
            " Epoch [065/100] | Loss_D: 0.556609 Loss_G: 22.241509 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [066/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.870042 Loss_G: 20.381420 | Valid_D: 1.307715 Fake_D: 0.432368 | BCE_G: 0.998289 Img_G: 0.128954 Per_G: 0.324388\n",
            " Batch [050/125] | Loss_D: 0.621226 Loss_G: 21.394863 | Valid_D: 0.820459 Fake_D: 0.421993 | BCE_G: 1.079607 Img_G: 0.135830 Per_G: 0.336613\n",
            " Batch [075/125] | Loss_D: 0.492079 Loss_G: 23.215967 | Valid_D: 0.339999 Fake_D: 0.644159 | BCE_G: 0.856559 Img_G: 0.152098 Per_G: 0.357482\n",
            " Batch [100/125] | Loss_D: 0.558634 Loss_G: 21.626902 | Valid_D: 0.379952 Fake_D: 0.737315 | BCE_G: 0.806438 Img_G: 0.140873 Per_G: 0.336659\n",
            " Batch [125/125] | Loss_D: 0.658131 Loss_G: 21.703476 | Valid_D: 0.907257 Fake_D: 0.409005 | BCE_G: 1.315355 Img_G: 0.135498 Per_G: 0.341918\n",
            " ----------------------------------------------------\n",
            " Epoch [066/100] | Loss_D: 0.555847 Loss_G: 22.250513 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [067/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.573920 Loss_G: 21.819515 | Valid_D: 0.621217 Fake_D: 0.526622 | BCE_G: 0.972592 Img_G: 0.136408 Per_G: 0.360307\n",
            " Batch [050/125] | Loss_D: 0.586227 Loss_G: 22.540630 | Valid_D: 0.451878 Fake_D: 0.720576 | BCE_G: 0.835637 Img_G: 0.153216 Per_G: 0.319171\n",
            " Batch [075/125] | Loss_D: 0.806469 Loss_G: 21.097399 | Valid_D: 1.243748 Fake_D: 0.369190 | BCE_G: 1.413051 Img_G: 0.128485 Per_G: 0.341793\n",
            " Batch [100/125] | Loss_D: 0.466435 Loss_G: 21.873102 | Valid_D: 0.508326 Fake_D: 0.424545 | BCE_G: 1.184198 Img_G: 0.141369 Per_G: 0.327600\n",
            " Batch [125/125] | Loss_D: 0.599908 Loss_G: 22.985123 | Valid_D: 0.447679 Fake_D: 0.752137 | BCE_G: 0.790895 Img_G: 0.153724 Per_G: 0.341091\n",
            " ----------------------------------------------------\n",
            " Epoch [067/100] | Loss_D: 0.558606 Loss_G: 22.019825 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [068/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.550845 Loss_G: 20.436615 | Valid_D: 0.676053 Fake_D: 0.425638 | BCE_G: 1.053117 Img_G: 0.133038 Per_G: 0.303984\n",
            " Batch [050/125] | Loss_D: 0.383985 Loss_G: 22.880201 | Valid_D: 0.348686 Fake_D: 0.419284 | BCE_G: 1.392000 Img_G: 0.149464 Per_G: 0.327090\n",
            " Batch [075/125] | Loss_D: 0.510897 Loss_G: 21.777473 | Valid_D: 0.438226 Fake_D: 0.583569 | BCE_G: 0.938879 Img_G: 0.144640 Per_G: 0.318732\n",
            " Batch [100/125] | Loss_D: 0.740762 Loss_G: 21.533899 | Valid_D: 0.952030 Fake_D: 0.529493 | BCE_G: 0.863372 Img_G: 0.149084 Per_G: 0.288108\n",
            " Batch [125/125] | Loss_D: 0.471846 Loss_G: 23.430426 | Valid_D: 0.335428 Fake_D: 0.608263 | BCE_G: 0.910820 Img_G: 0.154057 Per_G: 0.355696\n",
            " ----------------------------------------------------\n",
            " Epoch [068/100] | Loss_D: 0.549297 Loss_G: 21.967011 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [069/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.487351 Loss_G: 21.567009 | Valid_D: 0.405490 Fake_D: 0.569211 | BCE_G: 0.922715 Img_G: 0.136177 Per_G: 0.351328\n",
            " Batch [050/125] | Loss_D: 0.569233 Loss_G: 20.447439 | Valid_D: 0.493405 Fake_D: 0.645060 | BCE_G: 0.782977 Img_G: 0.136749 Per_G: 0.299477\n",
            " Batch [075/125] | Loss_D: 0.414095 Loss_G: 22.139790 | Valid_D: 0.386398 Fake_D: 0.441792 | BCE_G: 1.145641 Img_G: 0.145009 Per_G: 0.324663\n",
            " Batch [100/125] | Loss_D: 0.591820 Loss_G: 21.009188 | Valid_D: 0.763364 Fake_D: 0.420276 | BCE_G: 1.231130 Img_G: 0.132747 Per_G: 0.325170\n",
            " Batch [125/125] | Loss_D: 0.464264 Loss_G: 23.000811 | Valid_D: 0.478777 Fake_D: 0.449751 | BCE_G: 1.220094 Img_G: 0.149358 Per_G: 0.342244\n",
            " ----------------------------------------------------\n",
            " Epoch [069/100] | Loss_D: 0.547991 Loss_G: 21.836619 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [070/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.538246 Loss_G: 20.628498 | Valid_D: 0.361052 Fake_D: 0.715440 | BCE_G: 0.842169 Img_G: 0.135532 Per_G: 0.311658\n",
            " Batch [050/125] | Loss_D: 0.535720 Loss_G: 22.928738 | Valid_D: 0.659143 Fake_D: 0.412297 | BCE_G: 1.226022 Img_G: 0.150776 Per_G: 0.331257\n",
            " Batch [075/125] | Loss_D: 0.403709 Loss_G: 24.003906 | Valid_D: 0.334852 Fake_D: 0.472567 | BCE_G: 1.277779 Img_G: 0.164277 Per_G: 0.314922\n",
            " Batch [100/125] | Loss_D: 0.549451 Loss_G: 22.754276 | Valid_D: 0.358078 Fake_D: 0.740824 | BCE_G: 0.895140 Img_G: 0.152188 Per_G: 0.332015\n",
            " Batch [125/125] | Loss_D: 0.482469 Loss_G: 20.854836 | Valid_D: 0.525595 Fake_D: 0.439344 | BCE_G: 1.012779 Img_G: 0.134567 Per_G: 0.319267\n",
            " ----------------------------------------------------\n",
            " Epoch [070/100] | Loss_D: 0.545737 Loss_G: 21.837677 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [071/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.658856 Loss_G: 21.266973 | Valid_D: 0.904808 Fake_D: 0.412905 | BCE_G: 1.147272 Img_G: 0.138261 Per_G: 0.314682\n",
            " Batch [050/125] | Loss_D: 0.615306 Loss_G: 22.118334 | Valid_D: 0.603819 Fake_D: 0.626792 | BCE_G: 0.815086 Img_G: 0.147358 Per_G: 0.328371\n",
            " Batch [075/125] | Loss_D: 0.550376 Loss_G: 22.545259 | Valid_D: 0.341885 Fake_D: 0.758868 | BCE_G: 0.885657 Img_G: 0.150341 Per_G: 0.331275\n",
            " Batch [100/125] | Loss_D: 0.516562 Loss_G: 22.547882 | Valid_D: 0.625713 Fake_D: 0.407411 | BCE_G: 1.286132 Img_G: 0.146416 Per_G: 0.331008\n",
            " Batch [125/125] | Loss_D: 0.407607 Loss_G: 23.514538 | Valid_D: 0.453890 Fake_D: 0.361325 | BCE_G: 1.444533 Img_G: 0.153710 Per_G: 0.334948\n",
            " ----------------------------------------------------\n",
            " Epoch [071/100] | Loss_D: 0.539164 Loss_G: 21.710976 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [072/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.454383 Loss_G: 22.607239 | Valid_D: 0.373851 Fake_D: 0.534915 | BCE_G: 1.043914 Img_G: 0.149169 Per_G: 0.332319\n",
            " Batch [050/125] | Loss_D: 0.729074 Loss_G: 20.477249 | Valid_D: 0.337560 Fake_D: 1.120587 | BCE_G: 0.624539 Img_G: 0.137013 Per_G: 0.307573\n",
            " Batch [075/125] | Loss_D: 0.852111 Loss_G: 21.269726 | Valid_D: 1.089956 Fake_D: 0.614266 | BCE_G: 0.726824 Img_G: 0.137384 Per_G: 0.340226\n",
            " Batch [100/125] | Loss_D: 0.411118 Loss_G: 21.757133 | Valid_D: 0.448233 Fake_D: 0.374002 | BCE_G: 1.480594 Img_G: 0.141766 Per_G: 0.304997\n",
            " Batch [125/125] | Loss_D: 0.605641 Loss_G: 20.537117 | Valid_D: 0.666889 Fake_D: 0.544392 | BCE_G: 0.825495 Img_G: 0.133083 Per_G: 0.320168\n",
            " ----------------------------------------------------\n",
            " Epoch [072/100] | Loss_D: 0.545558 Loss_G: 21.599732 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [073/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.514783 Loss_G: 21.666796 | Valid_D: 0.598190 Fake_D: 0.431377 | BCE_G: 1.211322 Img_G: 0.140494 Per_G: 0.320304\n",
            " Batch [050/125] | Loss_D: 0.415383 Loss_G: 22.520344 | Valid_D: 0.486197 Fake_D: 0.344568 | BCE_G: 1.599620 Img_G: 0.145609 Per_G: 0.317993\n",
            " Batch [075/125] | Loss_D: 0.662570 Loss_G: 21.927015 | Valid_D: 0.334498 Fake_D: 0.990641 | BCE_G: 0.744588 Img_G: 0.150108 Per_G: 0.308581\n",
            " Batch [100/125] | Loss_D: 0.554164 Loss_G: 21.064226 | Valid_D: 0.717469 Fake_D: 0.390859 | BCE_G: 1.222624 Img_G: 0.134202 Per_G: 0.321071\n",
            " Batch [125/125] | Loss_D: 0.419644 Loss_G: 21.931952 | Valid_D: 0.384916 Fake_D: 0.454373 | BCE_G: 1.168759 Img_G: 0.141546 Per_G: 0.330430\n",
            " ----------------------------------------------------\n",
            " Epoch [073/100] | Loss_D: 0.533922 Loss_G: 21.493785 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [074/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.557152 Loss_G: 20.985615 | Valid_D: 0.753331 Fake_D: 0.360973 | BCE_G: 1.376364 Img_G: 0.135101 Per_G: 0.304960\n",
            " Batch [050/125] | Loss_D: 0.676053 Loss_G: 20.314556 | Valid_D: 0.886646 Fake_D: 0.465461 | BCE_G: 1.052256 Img_G: 0.131118 Per_G: 0.307526\n",
            " Batch [075/125] | Loss_D: 0.490097 Loss_G: 20.358646 | Valid_D: 0.534347 Fake_D: 0.445848 | BCE_G: 1.066131 Img_G: 0.133715 Per_G: 0.296053\n",
            " Batch [100/125] | Loss_D: 0.380318 Loss_G: 22.469484 | Valid_D: 0.373751 Fake_D: 0.386884 | BCE_G: 1.350343 Img_G: 0.149214 Per_G: 0.309888\n",
            " Batch [125/125] | Loss_D: 0.449663 Loss_G: 21.617092 | Valid_D: 0.468138 Fake_D: 0.431188 | BCE_G: 1.124792 Img_G: 0.143142 Per_G: 0.308905\n",
            " ----------------------------------------------------\n",
            " Epoch [074/100] | Loss_D: 0.528520 Loss_G: 21.515751 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [075/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.418786 Loss_G: 22.439842 | Valid_D: 0.446189 Fake_D: 0.391383 | BCE_G: 1.407084 Img_G: 0.146239 Per_G: 0.320441\n",
            " Batch [050/125] | Loss_D: 0.374725 Loss_G: 23.645941 | Valid_D: 0.340145 Fake_D: 0.409306 | BCE_G: 1.410935 Img_G: 0.156590 Per_G: 0.328802\n",
            " Batch [075/125] | Loss_D: 0.416416 Loss_G: 22.841110 | Valid_D: 0.357273 Fake_D: 0.475559 | BCE_G: 1.160117 Img_G: 0.151794 Per_G: 0.325081\n",
            " Batch [100/125] | Loss_D: 0.506629 Loss_G: 20.727440 | Valid_D: 0.372278 Fake_D: 0.640979 | BCE_G: 0.930332 Img_G: 0.136083 Per_G: 0.309442\n",
            " Batch [125/125] | Loss_D: 0.469010 Loss_G: 20.592999 | Valid_D: 0.495198 Fake_D: 0.442822 | BCE_G: 1.107697 Img_G: 0.130756 Per_G: 0.320483\n",
            " ----------------------------------------------------\n",
            " Epoch [075/100] | Loss_D: 0.525425 Loss_G: 21.311206 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [076/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.478078 Loss_G: 20.586140 | Valid_D: 0.360919 Fake_D: 0.595236 | BCE_G: 0.949213 Img_G: 0.134750 Per_G: 0.308098\n",
            " Batch [050/125] | Loss_D: 0.674349 Loss_G: 21.952101 | Valid_D: 0.349490 Fake_D: 0.999207 | BCE_G: 0.636777 Img_G: 0.145532 Per_G: 0.338108\n",
            " Batch [075/125] | Loss_D: 0.521764 Loss_G: 19.986479 | Valid_D: 0.509581 Fake_D: 0.533948 | BCE_G: 0.950919 Img_G: 0.126189 Per_G: 0.320831\n",
            " Batch [100/125] | Loss_D: 0.576432 Loss_G: 21.063873 | Valid_D: 0.740151 Fake_D: 0.412712 | BCE_G: 1.187906 Img_G: 0.136483 Per_G: 0.311385\n",
            " Batch [125/125] | Loss_D: 0.463504 Loss_G: 20.874920 | Valid_D: 0.509829 Fake_D: 0.417179 | BCE_G: 1.314626 Img_G: 0.134468 Per_G: 0.305673\n",
            " ----------------------------------------------------\n",
            " Epoch [076/100] | Loss_D: 0.518434 Loss_G: 21.398565 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [077/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.484060 Loss_G: 20.621998 | Valid_D: 0.445950 Fake_D: 0.522169 | BCE_G: 0.982936 Img_G: 0.134522 Per_G: 0.309341\n",
            " Batch [050/125] | Loss_D: 0.461151 Loss_G: 20.984919 | Valid_D: 0.445979 Fake_D: 0.476322 | BCE_G: 1.116463 Img_G: 0.133515 Per_G: 0.325850\n",
            " Batch [075/125] | Loss_D: 0.540355 Loss_G: 20.485796 | Valid_D: 0.462920 Fake_D: 0.617790 | BCE_G: 0.839437 Img_G: 0.136582 Per_G: 0.299409\n",
            " Batch [100/125] | Loss_D: 0.615019 Loss_G: 21.924196 | Valid_D: 0.337479 Fake_D: 0.892559 | BCE_G: 0.752183 Img_G: 0.148065 Per_G: 0.318277\n",
            " Batch [125/125] | Loss_D: 0.724266 Loss_G: 21.555412 | Valid_D: 1.010844 Fake_D: 0.437688 | BCE_G: 0.977246 Img_G: 0.140636 Per_G: 0.325729\n",
            " ----------------------------------------------------\n",
            " Epoch [077/100] | Loss_D: 0.518229 Loss_G: 21.263688 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [078/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.493108 Loss_G: 21.914459 | Valid_D: 0.334993 Fake_D: 0.651222 | BCE_G: 1.029648 Img_G: 0.142064 Per_G: 0.333923\n",
            " Batch [050/125] | Loss_D: 0.598436 Loss_G: 20.497908 | Valid_D: 0.839721 Fake_D: 0.357151 | BCE_G: 1.463519 Img_G: 0.128672 Per_G: 0.308361\n",
            " Batch [075/125] | Loss_D: 0.446806 Loss_G: 19.840046 | Valid_D: 0.419736 Fake_D: 0.473876 | BCE_G: 1.102617 Img_G: 0.125253 Per_G: 0.310604\n",
            " Batch [100/125] | Loss_D: 0.403602 Loss_G: 22.005795 | Valid_D: 0.359261 Fake_D: 0.447942 | BCE_G: 1.246495 Img_G: 0.144391 Per_G: 0.316010\n",
            " Batch [125/125] | Loss_D: 0.406853 Loss_G: 20.393444 | Valid_D: 0.421209 Fake_D: 0.392496 | BCE_G: 1.377100 Img_G: 0.131833 Per_G: 0.291654\n",
            " ----------------------------------------------------\n",
            " Epoch [078/100] | Loss_D: 0.513425 Loss_G: 21.261586 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [079/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.497702 Loss_G: 20.268045 | Valid_D: 0.615859 Fake_D: 0.379545 | BCE_G: 1.314246 Img_G: 0.130197 Per_G: 0.296704\n",
            " Batch [050/125] | Loss_D: 0.605533 Loss_G: 19.796097 | Valid_D: 0.796855 Fake_D: 0.414212 | BCE_G: 1.228747 Img_G: 0.124953 Per_G: 0.303601\n",
            " Batch [075/125] | Loss_D: 0.497456 Loss_G: 20.454350 | Valid_D: 0.423638 Fake_D: 0.571274 | BCE_G: 0.998898 Img_G: 0.134137 Per_G: 0.302088\n",
            " Batch [100/125] | Loss_D: 0.513650 Loss_G: 20.974499 | Valid_D: 0.376120 Fake_D: 0.651180 | BCE_G: 0.819761 Img_G: 0.133898 Per_G: 0.338247\n",
            " Batch [125/125] | Loss_D: 0.657355 Loss_G: 20.942604 | Valid_D: 0.346921 Fake_D: 0.967789 | BCE_G: 0.668868 Img_G: 0.136484 Per_G: 0.331269\n",
            " ----------------------------------------------------\n",
            " Epoch [079/100] | Loss_D: 0.503262 Loss_G: 21.264577 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [080/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.544420 Loss_G: 19.608698 | Valid_D: 0.639459 Fake_D: 0.449380 | BCE_G: 1.147938 Img_G: 0.125709 Per_G: 0.294492\n",
            " Batch [050/125] | Loss_D: 0.375144 Loss_G: 22.310335 | Valid_D: 0.352344 Fake_D: 0.397945 | BCE_G: 1.365094 Img_G: 0.147594 Per_G: 0.309292\n",
            " Batch [075/125] | Loss_D: 0.379315 Loss_G: 21.693073 | Valid_D: 0.350514 Fake_D: 0.408117 | BCE_G: 1.355057 Img_G: 0.139119 Per_G: 0.321304\n",
            " Batch [100/125] | Loss_D: 0.456013 Loss_G: 20.428167 | Valid_D: 0.376052 Fake_D: 0.535975 | BCE_G: 1.012093 Img_G: 0.132755 Per_G: 0.307030\n",
            " Batch [125/125] | Loss_D: 0.403888 Loss_G: 21.552265 | Valid_D: 0.387451 Fake_D: 0.420325 | BCE_G: 1.288855 Img_G: 0.142812 Per_G: 0.299110\n",
            " ----------------------------------------------------\n",
            " Epoch [080/100] | Loss_D: 0.487013 Loss_G: 21.214970 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [081/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.429409 Loss_G: 20.744083 | Valid_D: 0.455027 Fake_D: 0.403791 | BCE_G: 1.297078 Img_G: 0.133560 Per_G: 0.304548\n",
            " Batch [050/125] | Loss_D: 0.693673 Loss_G: 20.883167 | Valid_D: 0.344184 Fake_D: 1.043163 | BCE_G: 0.628735 Img_G: 0.139830 Per_G: 0.313572\n",
            " Batch [075/125] | Loss_D: 0.507805 Loss_G: 20.590576 | Valid_D: 0.341026 Fake_D: 0.674584 | BCE_G: 0.901178 Img_G: 0.137773 Per_G: 0.295607\n",
            " Batch [100/125] | Loss_D: 0.465988 Loss_G: 22.307463 | Valid_D: 0.519626 Fake_D: 0.412351 | BCE_G: 1.315835 Img_G: 0.140894 Per_G: 0.345112\n",
            " Batch [125/125] | Loss_D: 0.479044 Loss_G: 20.740002 | Valid_D: 0.427847 Fake_D: 0.530241 | BCE_G: 1.001566 Img_G: 0.134830 Per_G: 0.312772\n",
            " ----------------------------------------------------\n",
            " Epoch [081/100] | Loss_D: 0.480845 Loss_G: 21.149968 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [082/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.513144 Loss_G: 20.263895 | Valid_D: 0.422136 Fake_D: 0.604152 | BCE_G: 0.887033 Img_G: 0.133158 Per_G: 0.303051\n",
            " Batch [050/125] | Loss_D: 0.461082 Loss_G: 19.962593 | Valid_D: 0.573000 Fake_D: 0.349163 | BCE_G: 1.489718 Img_G: 0.126714 Per_G: 0.290075\n",
            " Batch [075/125] | Loss_D: 0.415174 Loss_G: 21.415041 | Valid_D: 0.418868 Fake_D: 0.411480 | BCE_G: 1.376488 Img_G: 0.139271 Per_G: 0.305574\n",
            " Batch [100/125] | Loss_D: 0.452819 Loss_G: 20.809559 | Valid_D: 0.538608 Fake_D: 0.367030 | BCE_G: 1.458750 Img_G: 0.131805 Per_G: 0.308517\n",
            " Batch [125/125] | Loss_D: 0.389204 Loss_G: 21.870018 | Valid_D: 0.347134 Fake_D: 0.431273 | BCE_G: 1.166987 Img_G: 0.143014 Per_G: 0.320080\n",
            " ----------------------------------------------------\n",
            " Epoch [082/100] | Loss_D: 0.468570 Loss_G: 21.140394 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [083/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.434693 Loss_G: 20.719282 | Valid_D: 0.380653 Fake_D: 0.488732 | BCE_G: 1.100485 Img_G: 0.136399 Per_G: 0.298947\n",
            " Batch [050/125] | Loss_D: 0.463147 Loss_G: 20.378624 | Valid_D: 0.525018 Fake_D: 0.401275 | BCE_G: 1.292986 Img_G: 0.129508 Per_G: 0.306740\n",
            " Batch [075/125] | Loss_D: 0.524813 Loss_G: 20.186672 | Valid_D: 0.513019 Fake_D: 0.536606 | BCE_G: 1.028971 Img_G: 0.131486 Per_G: 0.300454\n",
            " Batch [100/125] | Loss_D: 0.499556 Loss_G: 20.987411 | Valid_D: 0.482063 Fake_D: 0.517049 | BCE_G: 1.083544 Img_G: 0.133667 Per_G: 0.326859\n",
            " Batch [125/125] | Loss_D: 0.412552 Loss_G: 21.882437 | Valid_D: 0.416870 Fake_D: 0.408234 | BCE_G: 1.283663 Img_G: 0.143033 Per_G: 0.314774\n",
            " ----------------------------------------------------\n",
            " Epoch [083/100] | Loss_D: 0.457310 Loss_G: 21.194654 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [084/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.368604 Loss_G: 22.248867 | Valid_D: 0.342224 Fake_D: 0.394985 | BCE_G: 1.485948 Img_G: 0.146868 Per_G: 0.303805\n",
            " Batch [050/125] | Loss_D: 0.420520 Loss_G: 21.078321 | Valid_D: 0.408547 Fake_D: 0.432492 | BCE_G: 1.267755 Img_G: 0.136028 Per_G: 0.310388\n",
            " Batch [075/125] | Loss_D: 0.454135 Loss_G: 21.044415 | Valid_D: 0.512560 Fake_D: 0.395710 | BCE_G: 1.376641 Img_G: 0.128999 Per_G: 0.338395\n",
            " Batch [100/125] | Loss_D: 0.471675 Loss_G: 20.543230 | Valid_D: 0.358400 Fake_D: 0.584950 | BCE_G: 1.023971 Img_G: 0.135282 Per_G: 0.299551\n",
            " Batch [125/125] | Loss_D: 0.370183 Loss_G: 22.185173 | Valid_D: 0.371153 Fake_D: 0.369213 | BCE_G: 1.525123 Img_G: 0.141168 Per_G: 0.327164\n",
            " ----------------------------------------------------\n",
            " Epoch [084/100] | Loss_D: 0.454350 Loss_G: 21.128759 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [085/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.485740 Loss_G: 22.077463 | Valid_D: 0.337888 Fake_D: 0.633592 | BCE_G: 0.923501 Img_G: 0.146916 Per_G: 0.323117\n",
            " Batch [050/125] | Loss_D: 0.389882 Loss_G: 21.457584 | Valid_D: 0.430710 Fake_D: 0.349054 | BCE_G: 1.665245 Img_G: 0.135479 Per_G: 0.312221\n",
            " Batch [075/125] | Loss_D: 0.514045 Loss_G: 19.142916 | Valid_D: 0.497165 Fake_D: 0.530925 | BCE_G: 0.968722 Img_G: 0.124283 Per_G: 0.287294\n",
            " Batch [100/125] | Loss_D: 0.393191 Loss_G: 21.667858 | Valid_D: 0.344444 Fake_D: 0.441938 | BCE_G: 1.227664 Img_G: 0.142903 Per_G: 0.307493\n",
            " Batch [125/125] | Loss_D: 0.521990 Loss_G: 20.864347 | Valid_D: 0.635747 Fake_D: 0.408233 | BCE_G: 1.289351 Img_G: 0.136818 Per_G: 0.294660\n",
            " ----------------------------------------------------\n",
            " Epoch [085/100] | Loss_D: 0.444396 Loss_G: 21.229133 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [086/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.449923 Loss_G: 20.761631 | Valid_D: 0.441929 Fake_D: 0.457916 | BCE_G: 1.201882 Img_G: 0.139360 Per_G: 0.281189\n",
            " Batch [050/125] | Loss_D: 0.444159 Loss_G: 21.455299 | Valid_D: 0.354145 Fake_D: 0.534173 | BCE_G: 1.042188 Img_G: 0.142494 Per_G: 0.308183\n",
            " Batch [075/125] | Loss_D: 0.592073 Loss_G: 19.463619 | Valid_D: 0.847337 Fake_D: 0.336809 | BCE_G: 1.605901 Img_G: 0.122976 Per_G: 0.278006\n",
            " Batch [100/125] | Loss_D: 0.407192 Loss_G: 22.862076 | Valid_D: 0.345866 Fake_D: 0.468518 | BCE_G: 1.217718 Img_G: 0.152050 Per_G: 0.321969\n",
            " Batch [125/125] | Loss_D: 0.363051 Loss_G: 21.957645 | Valid_D: 0.361416 Fake_D: 0.364686 | BCE_G: 1.585043 Img_G: 0.142409 Per_G: 0.306583\n",
            " ----------------------------------------------------\n",
            " Epoch [086/100] | Loss_D: 0.436860 Loss_G: 21.120619 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [087/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.432613 Loss_G: 20.999136 | Valid_D: 0.513321 Fake_D: 0.351906 | BCE_G: 1.688676 Img_G: 0.132959 Per_G: 0.300730\n",
            " Batch [050/125] | Loss_D: 0.392924 Loss_G: 20.444290 | Valid_D: 0.412355 Fake_D: 0.373493 | BCE_G: 1.387086 Img_G: 0.131110 Per_G: 0.297312\n",
            " Batch [075/125] | Loss_D: 0.382251 Loss_G: 21.137011 | Valid_D: 0.360980 Fake_D: 0.403521 | BCE_G: 1.362952 Img_G: 0.134840 Per_G: 0.314501\n",
            " Batch [100/125] | Loss_D: 0.453611 Loss_G: 22.549480 | Valid_D: 0.355264 Fake_D: 0.551958 | BCE_G: 1.033657 Img_G: 0.151032 Per_G: 0.320629\n",
            " Batch [125/125] | Loss_D: 0.424942 Loss_G: 21.094170 | Valid_D: 0.376881 Fake_D: 0.473002 | BCE_G: 1.153725 Img_G: 0.134453 Per_G: 0.324755\n",
            " ----------------------------------------------------\n",
            " Epoch [087/100] | Loss_D: 0.450641 Loss_G: 21.157508 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [088/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.480762 Loss_G: 20.485260 | Valid_D: 0.523152 Fake_D: 0.438371 | BCE_G: 1.216733 Img_G: 0.132048 Per_G: 0.303188\n",
            " Batch [050/125] | Loss_D: 0.563914 Loss_G: 18.952597 | Valid_D: 0.730397 Fake_D: 0.397431 | BCE_G: 1.404192 Img_G: 0.121624 Per_G: 0.269303\n",
            " Batch [075/125] | Loss_D: 0.401497 Loss_G: 21.383606 | Valid_D: 0.349275 Fake_D: 0.453719 | BCE_G: 1.168684 Img_G: 0.141371 Per_G: 0.303889\n",
            " Batch [100/125] | Loss_D: 0.441431 Loss_G: 20.775038 | Valid_D: 0.398851 Fake_D: 0.484011 | BCE_G: 1.128391 Img_G: 0.133928 Per_G: 0.312693\n",
            " Batch [125/125] | Loss_D: 0.395543 Loss_G: 22.079105 | Valid_D: 0.343628 Fake_D: 0.447457 | BCE_G: 1.273299 Img_G: 0.143601 Per_G: 0.322286\n",
            " ----------------------------------------------------\n",
            " Epoch [088/100] | Loss_D: 0.466475 Loss_G: 21.107349 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [089/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.532234 Loss_G: 20.483503 | Valid_D: 0.716175 Fake_D: 0.348292 | BCE_G: 1.676689 Img_G: 0.126705 Per_G: 0.306814\n",
            " Batch [050/125] | Loss_D: 0.363251 Loss_G: 22.167702 | Valid_D: 0.338091 Fake_D: 0.388410 | BCE_G: 1.477976 Img_G: 0.144242 Per_G: 0.313278\n",
            " Batch [075/125] | Loss_D: 0.435696 Loss_G: 20.377035 | Valid_D: 0.436523 Fake_D: 0.434870 | BCE_G: 1.233882 Img_G: 0.132481 Per_G: 0.294754\n",
            " Batch [100/125] | Loss_D: 0.469765 Loss_G: 20.581009 | Valid_D: 0.488095 Fake_D: 0.451434 | BCE_G: 1.251164 Img_G: 0.131269 Per_G: 0.310148\n",
            " Batch [125/125] | Loss_D: 0.883468 Loss_G: 21.414957 | Valid_D: 1.416550 Fake_D: 0.350387 | BCE_G: 1.654799 Img_G: 0.136508 Per_G: 0.305468\n",
            " ----------------------------------------------------\n",
            " Epoch [089/100] | Loss_D: 0.458058 Loss_G: 21.109178 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [090/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.497551 Loss_G: 20.402290 | Valid_D: 0.554358 Fake_D: 0.440745 | BCE_G: 1.195156 Img_G: 0.131366 Per_G: 0.303527\n",
            " Batch [050/125] | Loss_D: 0.459848 Loss_G: 21.198168 | Valid_D: 0.528559 Fake_D: 0.391137 | BCE_G: 1.476185 Img_G: 0.133692 Per_G: 0.317637\n",
            " Batch [075/125] | Loss_D: 0.531747 Loss_G: 20.778595 | Valid_D: 0.644553 Fake_D: 0.418941 | BCE_G: 1.219692 Img_G: 0.132434 Per_G: 0.315774\n",
            " Batch [100/125] | Loss_D: 0.394221 Loss_G: 21.537537 | Valid_D: 0.373590 Fake_D: 0.414853 | BCE_G: 1.356035 Img_G: 0.140828 Per_G: 0.304937\n",
            " Batch [125/125] | Loss_D: 0.431061 Loss_G: 22.386982 | Valid_D: 0.411998 Fake_D: 0.450123 | BCE_G: 1.230609 Img_G: 0.143431 Per_G: 0.340666\n",
            " ----------------------------------------------------\n",
            " Epoch [090/100] | Loss_D: 0.470009 Loss_G: 21.048943 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [091/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.500179 Loss_G: 20.890568 | Valid_D: 0.342760 Fake_D: 0.657598 | BCE_G: 1.043509 Img_G: 0.133219 Per_G: 0.326257\n",
            " Batch [050/125] | Loss_D: 0.409475 Loss_G: 20.356327 | Valid_D: 0.412083 Fake_D: 0.406868 | BCE_G: 1.324763 Img_G: 0.127833 Per_G: 0.312415\n",
            " Batch [075/125] | Loss_D: 0.472492 Loss_G: 21.263025 | Valid_D: 0.350782 Fake_D: 0.594202 | BCE_G: 0.983692 Img_G: 0.140526 Per_G: 0.311337\n",
            " Batch [100/125] | Loss_D: 0.484467 Loss_G: 20.044983 | Valid_D: 0.589219 Fake_D: 0.379714 | BCE_G: 1.505815 Img_G: 0.123329 Per_G: 0.310314\n",
            " Batch [125/125] | Loss_D: 0.492322 Loss_G: 18.972279 | Valid_D: 0.582189 Fake_D: 0.402455 | BCE_G: 1.332927 Img_G: 0.116381 Per_G: 0.300062\n",
            " ----------------------------------------------------\n",
            " Epoch [091/100] | Loss_D: 0.466476 Loss_G: 20.865411 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [092/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.461289 Loss_G: 21.417355 | Valid_D: 0.340561 Fake_D: 0.582016 | BCE_G: 1.016687 Img_G: 0.141487 Per_G: 0.312600\n",
            " Batch [050/125] | Loss_D: 0.404204 Loss_G: 20.793781 | Valid_D: 0.382755 Fake_D: 0.425653 | BCE_G: 1.316708 Img_G: 0.133747 Per_G: 0.305116\n",
            " Batch [075/125] | Loss_D: 0.462765 Loss_G: 22.373022 | Valid_D: 0.411123 Fake_D: 0.514407 | BCE_G: 1.046366 Img_G: 0.150391 Per_G: 0.314376\n",
            " Batch [100/125] | Loss_D: 0.460242 Loss_G: 20.413467 | Valid_D: 0.344002 Fake_D: 0.576482 | BCE_G: 1.074587 Img_G: 0.128516 Per_G: 0.324363\n",
            " Batch [125/125] | Loss_D: 0.407247 Loss_G: 21.855808 | Valid_D: 0.374008 Fake_D: 0.440486 | BCE_G: 1.268260 Img_G: 0.141399 Per_G: 0.322382\n",
            " ----------------------------------------------------\n",
            " Epoch [092/100] | Loss_D: 0.444985 Loss_G: 20.780714 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [093/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.530051 Loss_G: 20.194530 | Valid_D: 0.360710 Fake_D: 0.699392 | BCE_G: 0.871334 Img_G: 0.131789 Per_G: 0.307214\n",
            " Batch [050/125] | Loss_D: 0.533711 Loss_G: 19.431648 | Valid_D: 0.660438 Fake_D: 0.406984 | BCE_G: 1.318094 Img_G: 0.121385 Per_G: 0.298752\n",
            " Batch [075/125] | Loss_D: 0.476679 Loss_G: 20.124172 | Valid_D: 0.535000 Fake_D: 0.418359 | BCE_G: 1.217390 Img_G: 0.130080 Per_G: 0.294938\n",
            " Batch [100/125] | Loss_D: 0.445758 Loss_G: 21.670660 | Valid_D: 0.404629 Fake_D: 0.486887 | BCE_G: 1.131904 Img_G: 0.141711 Per_G: 0.318382\n",
            " Batch [125/125] | Loss_D: 0.386907 Loss_G: 20.280449 | Valid_D: 0.373708 Fake_D: 0.400106 | BCE_G: 1.384686 Img_G: 0.128743 Per_G: 0.301076\n",
            " ----------------------------------------------------\n",
            " Epoch [093/100] | Loss_D: 0.439870 Loss_G: 20.718537 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [094/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.521623 Loss_G: 18.828087 | Valid_D: 0.655319 Fake_D: 0.387927 | BCE_G: 1.437855 Img_G: 0.116172 Per_G: 0.288649\n",
            " Batch [050/125] | Loss_D: 0.398813 Loss_G: 22.678007 | Valid_D: 0.345260 Fake_D: 0.452366 | BCE_G: 1.236080 Img_G: 0.151683 Per_G: 0.313681\n",
            " Batch [075/125] | Loss_D: 0.475385 Loss_G: 19.680384 | Valid_D: 0.367993 Fake_D: 0.582776 | BCE_G: 1.057576 Img_G: 0.129209 Per_G: 0.285097\n",
            " Batch [100/125] | Loss_D: 0.403714 Loss_G: 22.292057 | Valid_D: 0.341961 Fake_D: 0.465468 | BCE_G: 1.235369 Img_G: 0.147994 Per_G: 0.312865\n",
            " Batch [125/125] | Loss_D: 0.485321 Loss_G: 20.194740 | Valid_D: 0.500684 Fake_D: 0.469959 | BCE_G: 1.093889 Img_G: 0.126918 Per_G: 0.320454\n",
            " ----------------------------------------------------\n",
            " Epoch [094/100] | Loss_D: 0.429568 Loss_G: 20.621688 (Epoch Average)\n",
            " \n",
            "\n",
            " Epoch [095/100]\n",
            " ----------------------------------------------------\n",
            " Batch [025/125] | Loss_D: 0.433929 Loss_G: 19.366488 | Valid_D: 0.460750 Fake_D: 0.407107 | BCE_G: 1.317114 Img_G: 0.123645 Per_G: 0.284246\n",
            " Batch [050/125] | Loss_D: 0.434475 Loss_G: 20.637865 | Valid_D: 0.491145 Fake_D: 0.377805 | BCE_G: 1.366099 Img_G: 0.130924 Per_G: 0.308968\n",
            " Batch [075/125] | Loss_D: 0.411779 Loss_G: 19.520756 | Valid_D: 0.387955 Fake_D: 0.435604 | BCE_G: 1.379517 Img_G: 0.124874 Per_G: 0.282693\n",
            " Batch [100/125] | Loss_D: 0.407198 Loss_G: 20.817852 | Valid_D: 0.425123 Fake_D: 0.389274 | BCE_G: 1.495850 Img_G: 0.131069 Per_G: 0.310753\n",
            " Batch [125/125] | Loss_D: 0.378025 Loss_G: 20.926254 | Valid_D: 0.400213 Fake_D: 0.355836 | BCE_G: 1.648635 Img_G: 0.132636 Per_G: 0.300699\n",
            " ----------------------------------------------------\n",
            " Epoch [095/100] | Loss_D: 0.432495 Loss_G: 20.567179 (Epoch Average)\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "START_EPOCH = 1\n",
        "EPOCHS_TO_TRAIN = 100\n",
        "LAST_EPOCH = START_EPOCH + EPOCHS_TO_TRAIN - 1\n",
        "STEPS_PER_EPOCH = len(dataloader)\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/SAR_COLOR/checkpoints2'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "loss_Gs = []\n",
        "loss_Ds = []\n",
        "\n",
        "for epoch in range(START_EPOCH, LAST_EPOCH + 1):\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_loss_G = 0.0\n",
        "    epoch_loss_D = 0.0\n",
        "\n",
        "    # progress_bar = tqdm(enumerate(dataloader), desc=f'Epoch [{epoch:03}/{LAST_EPOCH:03}]', leave=True)\n",
        "\n",
        "    print(f'\\nEpoch [{epoch:03}/{LAST_EPOCH:03}]')\n",
        "    print('-' * 52)\n",
        "\n",
        "    # for i, (sar, col, _) in tqdm:\n",
        "    for i, (sar, col, _) in enumerate(dataloader):\n",
        "\n",
        "        sar = sar.to(DEVICE)\n",
        "        col = col.to(DEVICE)\n",
        "\n",
        "        fake = generator(sar)\n",
        "\n",
        "        for param in discriminator.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        patch_valid = discriminator(sar, col)\n",
        "        patch_fake = discriminator(sar, fake.detach())\n",
        "        loss_D, valid_loss_D, fake_loss_D = discriminator_loss(patch_fake, patch_valid)\n",
        "        loss_D.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
        "        optimizer_D.step()\n",
        "\n",
        "        for param in discriminator.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        patch_fake = discriminator(sar, fake)\n",
        "        loss_G, bce_loss_G, img_loss_G, per_loss_G = generator_loss(fake, col, patch_fake)\n",
        "        loss_G.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # progress_bar.set_postfix({\n",
        "        #     \"G Loss\": f'{loss_G.item():.6f}',\n",
        "        #     \"D Loss\": f'{loss_D.item():.6f}',\n",
        "        # })\n",
        "\n",
        "        # if (i + 1) % 1 == 0:\n",
        "        if (i + 1) % (STEPS_PER_EPOCH // 5) == 0:\n",
        "            print(\n",
        "                f\"Batch [{i+1:03}/{STEPS_PER_EPOCH:03}] | \"\n",
        "                f\"Loss_D: {loss_D.item():8.6f} Loss_G: {loss_G.item():9.6f} | \"\n",
        "                f\"Valid_D: {valid_loss_D:8.6f} Fake_D: {fake_loss_D:8.6f} | \"\n",
        "                f\"BCE_G: {bce_loss_G:8.6f} Img_G: {img_loss_G:8.6f} Per_G: {per_loss_G:8.6f}\"\n",
        "            )\n",
        "\n",
        "        loss_Gs.append(loss_G.item())\n",
        "        loss_Ds.append(loss_D.item())\n",
        "\n",
        "        epoch_loss_D += loss_D.item()\n",
        "        epoch_loss_G += loss_G.item()\n",
        "\n",
        "    epoch_loss_D /= STEPS_PER_EPOCH\n",
        "    epoch_loss_G /= STEPS_PER_EPOCH\n",
        "\n",
        "    print('-' * 52)\n",
        "    print(\n",
        "        f\"Epoch [{epoch:03}/{LAST_EPOCH:03}] | \"\n",
        "        f\"Loss_D: {epoch_loss_D:8.6f} Loss_G: {epoch_loss_G:9.6f} (Epoch Average)\\n\"\n",
        "    )\n",
        "\n",
        "    # if epoch % 3 == 0:\n",
        "    #     visualize_results(sar[0], fake[0], col[0])\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        'loss_G': loss_G.item(),\n",
        "        'loss_D': loss_D.item(),\n",
        "    }\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch:03}.pth')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        if epoch > 29:\n",
        "            checkpoint_path_del = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch - 25:03}.pth')\n",
        "            os.remove(checkpoint_path_del)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaT7FIr6NiGd"
      },
      "outputs": [],
      "source": [
        "# from torchviz import make_dot\n",
        "\n",
        "# dot = make_dot(loss_D, params=dict(discriminator.named_parameters()))\n",
        "# dot.render(\"gradient_flow_D\", format=\"png\")\n",
        "# dot.view()\n",
        "\n",
        "# dot = make_dot(loss_G, params=dict(generator.named_parameters()))\n",
        "# dot.render(\"gradient_flow_G\", format=\"png\")\n",
        "# dot.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTfHv2ojYA6H"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_Gs)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Generator Loss')\n",
        "# plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0HfDyCTYA6I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_Ds)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Discriminator Loss')\n",
        "# plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8BVRmq7ecM5"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
